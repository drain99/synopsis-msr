\chapter{Evaluation}
\label{sec:eval}
\vspace{-10px}
We have implemented \toolName{} on top of the Counter tool \cite{oopsla20}.
We use {\em four} SMT solvers running in parallel for solving
SMT proof obligations discharged by our proof discharge algorithm:
{\tt z3-4.8.7}, {\tt z3-4.8.14} \cite{z3}, {\tt Yices2-45e38fc} \cite{yices}, and {\tt cvc4-1.7} \cite{cvc4solver}.
An unroll factor of {\em four} is used to handle loop unrolling in the C implementation.
We use a default value of {\em eight} for over- and under-approximation depths ($d_o$ and $d_u$).
The default values for unrolling parameter $k$ (used for categorization of proof obligations)
and $\eta$ (used by {\tt strongestInvCover()} during weakening of \recursiveRelation{} invariants) are {\em five}.

\toolName{} requires the user to provide a \SpecL{} program $S$ (specification), a C implementation $C$,
and a file that contains their input-output specifications.
For each function pair, \toolName{} attempts to find equivalence between their CFGs \sprog{} and \cprog{}
under their respective \pre{} and \post{} (given as part of input-output specification).
An equivalence check requires the identification of lifting constructors to relate \cprog{}
values to the ADT values in \sprog{} through  \recursiveRelations{}.
Such relations may be required at the entry of both programs (i.e. in the precondition \pre{}),
in the middle of both programs (i.e. as invariants at intermediate product-CFG nodes),
and at the exit of both programs (i.e. in the postcondition \post{}).
\pre{} and \post{} are user-specified, whereas the inductive invariants are
inferred automatically by our algorithm.
During invariant inference, \toolName{} derives the candidate lifting constructors
from the user-specified \pre{} and \post{}.
More sophisticated approaches to finding lifting constructors are left as future work.

\section{Experiments}
\label{sec:experiments}
\vspace{-5px}
We consider programs involving four distinct ADTs, namely,
\curvedtype{T1} \type{Str}, \curvedtype{T2} \type{List}, \curvedtype{T3} \type{Tree},
and \curvedtype{T4} \type{Matrix}.
For each \SpecL{} program specification, we consider multiple
C implementations that differ in their (a) memory layout of ADTs, and
(b) algorithmic strategies. For example, a \type{Matrix} in C may be laid out
in a two-dimensional array, a one-dimensional array using row or column major
layouts etc. On the other hand, an optimized implementation may choose manual vectorization
of an inner-most loop. Next, we consider each ADT in more detail. For each,
we discuss (a) its corresponding programs, (b) C memory layouts and their lifting
constructors, and (c) varying algorithmic strategies.
All \SpecL{} specifications are written based on naive algorithms that simply confirm to the function specifications.
Importantly, no consideration was given to the actual C implementations during the design of specifications.
\input{chapters/figures/figThesisLiftingConsStr.tex}
\vspace{-5px}
\subsection{String}
\label{sec:expstring}
\vspace{-5px}
We wrote a single specification in \SpecL{} for each of the following
common string library functions: {\tt strlen}, {\tt strchr}, {\tt strcmp}, {\tt strspn},
{\tt strcspn}, and {\tt strpbrk}.  For each specification
program, we took multiple C implementations of that program, drawn from popular
libraries like {\tt glibc} \cite{glibc}, {\tt klibc} \cite{klibc}, {\tt newlib} \cite{newlib},
{\tt openbsd} \cite{openbsdlibc}, {\tt uClibc} \cite{uclibc},
{\tt dietlibc} \cite{dietlibc}, {\tt musl} \cite{musl}, and {\tt netbsd} \cite{netbsd}.
% Some of these libraries implement the same function in two ways: one that is optimized
% for code size and another that is optimized for runtime.
These library implementations use a nul-terminated array to represent
a string, and the corresponding lifting constructor is \lift{str}{\mem{}}{u8[]}.
\type{u<N>} represents the N-bit unsigned integer type in C.
For example, \type{u8} represents \type{unsigned char} type.

Further, we implemented custom C programs for these functions that uses linked list
and {\em chunked linked list} data structures to represent a string.
In a chunked linked list, a single list node (linked through a {\tt next} pointer)
contains a small array (chunk) of values.
We use a default chunk size of four for our benchmarks.
The corresponding lifting constructors are \lift{str}{\mem{}}{lnode(u8)}
and \lift{str}{\mem{}}{clnode(u8)} respectively.
These lifting constructors are defined in \cref{tab:LiftingConsStr}.
\lift{str}{\mem{}}{lnode(u8)} requires a single
argument $p$ representing the pointer to the list node.
On the other hand, \lift{str}{\mem{}}{clnode(u8)} requires two arguments $p$
and $i$, where $p$ represents the pointer to the chunked linked list node
and $i$ represents the position of the initial character in the chunk.
\input{chapters/figures/figStrchrSpecAndC.tex}
\subsubsection{An Example : strchr}
\label{sec:strchrexample}
Additionally, we define an optional string type \type{OptStr} to specify
behaviour of functions that conditionally return a string (e.g. {\tt strchr}, {\tt strpbrk}).
The \type{OptStr} ADT along with three (pairs of) lifting constructors for the three layouts of the \type{Str} ADT
are shown in \cref{tab:LiftingConsStr}.
{\tt strchr} accepts a string $t$ and a character $c$\footnote{Due to historical reasons,
the type of $c$ is declared as \type{int}
to maintain backward compatibility with pre C-98 code.
However, the function is specified to cast it to a character and use it instead.} and returns
the longest suffix of $t$ that begins with $c$, otherwise it returns the null pointer to indicate
failure to find $c$ in the string $t$.
In case $c$ is the null character (i.e. nul), {\tt strchr} is defined to return the empty string (instead of the null pointer).
\Cref{fig:llStrchrSpecIR,fig:llStrchrCArrIR} show the IRs of the {\tt strchr} specification and
a generic C implementation respectively.
We demonstrate two important aspects of \toolName{} using this example -- (a) use of \sdef{} and \pre{} to constrain the C implementation
to only well-formed inputs (in \cref{sec:cinputcons}),
and (b) need for correlating pathsets (instead of paths) (in \cref{sec:pathsetcorrel}).

Recall that a nul-terminated C string is only well-formed if the string itself does not belong to a region of memory containing the null pointer.
This well-formedness condition is necessary to prove that the pointer to the string returned at \cpc{6} (in \cref{fig:llStrchrCArrIR})
is non-null (used uniquely to indicate a failure to find the character $c$ in the string $t$).
As previously discussed in \cref{sec:eqdef}, we expose this well-formedness condition in the specification using
the explicit \type{Str} data constructor \cons{SInvalid}.
Finally, we assert that \sv{s} in \cref{fig:llStrchrSpecIR} is well-formed using the {\tt assuming-do} statement
(\spc{3} in \cref{fig:llStrchrSpecIR}) and relate the non-null well-formedness condition of the C input string \cv{t}
with the condition of \sv{s} being \cons{SInvalid} using \pre{} (labeled \circled{\footnotesize P1} in \cref{fig:llStrchrInvs}).
Note the use of \lift{optstr}{\mem{}}{u8[]} in the postcondition (labeled \circled{E} in \cref{fig:llStrchrInvs}).

\Cref{fig:llStrchrProductCFG} shows the product-CFG showing the path correlations between \sprog{} and \cprog{}.
Consider the product-CFG edge \scedge{1}{2}{E}{E} correlating the pathsets:
{\small $\pathset{S1,S3,(S4 \pathpar (\pathset{S4,S5}) \pathpar S7), SE}$} (in \sprog{}) and
{\small $\pathset{C2,((\pathset{C3,C4}) \pathpar C6),CE}$} (in \cprog{}).
The $\rightarrow$ and $\pathpar$ operators are used to represent `series' and `parallel' path combinations.
The above two pathsets represent the following two sets
{\small $\{ \spath{1,3,4,E},\ \spath{1,3,4,5,E},\ \spath{1,3,7,E} \}$} and
{\small $\{ \cpath{2,3,4,E},\ \cpath{2,6,E} \}$} respectively.
In \sprog{}, the case of \sv{c} being nul is handled explicitly at \spc{4}, whereas
\spc{7} handles the case of \sv{s} containing the (non-nul) character \sv{c}.
Interestingly in \cprog{}, both these cases are taken care of by the singular exit edge outgoing at \cpc{6}.
This is an example where a path in \cprog{} (i.e. \cpath{2,6,E}) is simultaneously active with
two paths in \sprog{} (i.e. \spath{1,3,4,E} and \spath{1,3,7,E}).
Evidently, for a successful bisimulation proof, we are required to correlate the \cprog{} path
\cpath{2,6,E} with the \sprog{} pathset $\{ \spath{1,3,4,E}, \spath{1,3,7,E} \}$.
Such situations are rather frequent because the strongly-typed specification is forced to handle
each case explicitly while a C implementation may take advantage of the
underlying representation to generalize multiple explicit cases into one.
\input{chapters/figures/figStrlenSpecAndC.tex}
\subsubsection{Another Example : strlen}
\Cref{fig:strlenSpecAndC} shows the {\tt strlen} specification and two vastly
different $C$ implementations. \Cref{fig:llStrlenCArrIR} is a generic implementation
using a C-style nul-terminated array to represent a string.
The second implementation in \cref{fig:llStrlenCClistIR} differs from \cref{fig:llStrlenCArrIR}
in the following: (a) it uses a chunked linked list data layout for the input string
and (b) it uses specialized bit manipulations to identify whether a (4 byte) chunk contains the nul character simultaneously.
\toolName{} is able to automatically find a bisimulation relation
for both implementations against the unaltered specification.
\Cref{fig:StrlenProductCFGsAndInvs} shows the product-CFG and its associated node invariants for each implementation.
\input{chapters/figures/figStrlenCfgsWithInvs.tex}

Lifting constructors are named based on the C data layout being lifted
and the type of the lifted value.
For example, \lift{str}{}{u8[]} represents a \type{Str} lifting constructor
for an array layout.
In general, we use the following naming convention for different C data layouts:
\type{T[]} represents an array of type \type{T} (e.g., \type{u8[]}).
\type{lnode(T)} represents a linked list node type containing a value of type \type{T}.
Similarly, \type{clnode(T)} and \type{tnode(T)} represent a chunked linked list and a tree node
with values of type \type{T} respectively.

\subsection{List}
\label{sec:explist}
We wrote a \SpecL{} program specification that creates a list, a
program that traverses a list to compute the sum of its elements and a program
that computes the dot product of two lists. We use three different
data layouts for a list in C: array (\lift{list}{\mem{}}{u32[]}),
linked list (\lift{list}{\mem{}}{lnode(u32)}), and
a chunked linked list (\lift{list}{\mem{}}{clnode(u32)}).
The lifting constructors are shown in \cref{tab:LiftingConsList}.
Although similar to the string lifting constructors, these lifting
constructors differ widely in their data encoding. For example,
\lifted{list}{\mem{}}{u32[]}{p,i,n} represents a \type{List} value constructed
from a C array of size $n$, pointed to by $p$ and starting at the $i^{th}$ index.
The list becomes empty when we are at the end of the array.
\lift{list}{\mem{}}{lnode(u32)} and \lift{list}{\mem{}}{clnode(u32)}, on the other hand, encodes the
empty list (i.e. \cons{LNil}) using the null pointer.
These layouts are in contrast to the \type{Str} layouts, all of which uses the nul character
to indicate the empty string.
\input{chapters/figures/figThesisLiftingConsList.tex}
\input{chapters/figures/figThesisLiftingConsTree.tex}
\subsection{Tree}
\label{sec:exptree}
We wrote a \SpecL{} program that computes the sum of the elements of a binary tree
through an inorder traversal using recursion. We use two different data layouts for a tree: 
(a) a flat array where a
{\em complete} binary tree is laid out in breadth-first search order commonly used for heaps (\lift{tree}{\mem{}}{u32[]}),
and (b) a linked tree node with two pointers for the left and right children (\lift{tree}{\mem{}}{tnode(u32)}) (shown in \cref{tab:LiftingConsTree}).
Both \SpecL{} and C programs contain non-tail recursive procedure calls for left and right children.
\toolName{} is able to correlate these recursive calls using user-provided \pre{} and \post{} as discussed in \cref{sec:correlfcalls}.
At the entry of the recursive calls, \toolName{} is required to prove that \pre{} holds for the arguments
and at the exit of the recursive calls, \toolName{} assumes \post{} on the values returned.
\input{chapters/figures/figThesisLiftingConsMatrix.tex}
\subsection{Matrix}
\label{sec:expmat}
\vspace{-5px}
We wrote a \SpecL{} program to count the frequency of a value appearing in a 2D matrix.
A matrix is represented as an ADT that resembles a \type{List} of \type{List}s (\inv{\small T4} in \cref{tab:LiftingConsMatrix}).
The C implementations for a \type{Matrix} object include
(a) a two-dimensional array (\lift{mat}{\mem{}}{u32[][]}), (b) a flattened row-major array (\lift{mat}{\mem{}}{u32[r]}),
(c) a flattened column-major array (\lift{mat}{\mem{}}{u32[c]}), (d) a linked list of 1D arrays (\lift{mat}{\mem{}}{lnode(u32[])}),
(e) a 1D array of linked lists (\lift{mat}{\mem{}}{lnode(u32)[]}) and (f) a 1D array of chunked linked list (\lift{mat}{\mem{}}{clnode(u32)[]})
memory layouts. Note that both \type{T[r]} and \type{T[c]} represent a 1D array of type {\tt T}; {\em r} and {\em c}
emphasizes that these arrays are used to represent matrices in row-major and column-major encodings respectively.
We also introduce two auxiliary lifting constructors \lift{list}{\mem{}}{u32[r]} and \lift{list}{\mem{}}{u32[c]}
for lifting each row of matrices lifted using the corresponding \lift{mat}{\mem{}}{u32[r]} and \lift{mat}{\mem{}}{u32[c]} \type{Matrix} lifting
constructors. These lifting constructors are listed in \cref{tab:LiftingConsMatrix}.

\input{chapters/figures/figTestsTableA.tex}
\input{chapters/figures/figTestsTableB.tex}
\begin{figure}
\begin{subfigure}[b]{0.43\textwidth}
\begin{center}
\begin{footnotesize}
\begin{tabular}{@{\hspace{1mm}}c@{\hspace{2mm}}r@{\hspace{2mm}}r@{\hspace{2mm}}r@{\hspace{2mm}}r@{\hspace{1mm}}}
\toprule
& \multicolumn{1}{c}{$\mathbf{Proven}$} & \multicolumn{1}{c}{$\mathbf{Disproven}$} & \multicolumn{1}{c}{$\mathbf{Unknown}$} & \multicolumn{1}{c}{$\bm{\Sigma}$} \\
\toprule
\multirow{2}{*}{\qcount{I}}    & 13      & 6                                       & \multicolumn{1}{r}{\multirow{2}{*}{0}}    & 19       \\
                               & 68.4\%  & 31.6\%                                  &                                           & 0.02\%   \\
\midrule
\multirow{2}{*}{\qcount{II}}   & 64966   & 1407                                    & 133                                       & 66506    \\
                               & 97.7\%  & 2.1\%                                   & 0.2\%                                     & 62.42\%  \\
\midrule
\multirow{2}{*}{\qcount{III}}  & 32588   & \multicolumn{1}{r}{\multirow{2}{*}{-}}  & 7441                                      & 40029    \\
                               & 81.4\%  &                                         & 18.6\%                                    & 37.56\%  \\
\midrule
\multicolumn{4}{l}{Total number of queries: }                                                                                  & 106554   \\
\bottomrule
\end{tabular}
\end{footnotesize}
\end{center}
\vspace{10px}
\caption{\label{tab:querycountstable}Total number of queries discharged by our solver during equivalence checks for the tests listed in \cref{tab:testsa,tab:testsb}.
We show the relative distribution of returned results for each type of query (\qcount{I}, \qcount{II} and \qcount{III}).}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{0.55\textwidth}
\begin{center}
\includegraphics[width=1.02\textwidth]{chapters/figures/figQueryDepthDist.png}
\end{center}
\caption{\label{tab:querydepthdist}Minimum over- and under-approximation depths at which approximate queries were successfully proven {\em or} disproven with counterexamples by our solver.
Approximate queries include type II prove, disprove queries and type III prove queries.}
\end{subfigure}
\end{figure}

\section{Results}
\label{sec:results}
\Cref{tab:testsa,tab:testsb} lists the various C implementations and the time \toolName{} took
to compute equivalence against their specifications.
For functions that take two or more data structures as arguments, we show
results for different combinations of data layouts for each argument.
We use \qcount{I}, \qcount{II} and \qcount{III} to represent the number of type I, II and III
queries discharged by our solver respectively.
\btcount{} denotes the number of times our counterexample-guided best-first search (described in \cref{sec:searchalgo}) backtracked
during the search for a product-CFG representing equivalence.
A higher value of $\mathcal{P}_{BT}$ represents that the search backtracked multiple times due to a combination of large search space
and a sub-optimal choice in best-first ranking.
We also show the minimum over-approximation (\minapprox{o}) and under-approximation (\minapprox{u}) depths
at which the equivalence proofs successfully completed (keeping all other parameters to their
default values).
\Cref{tab:querycountstable} shows a breakdown of the number of queries discharged by our solver by query type and returned result.
Our solver may either return {\em proven} implying a successfully proven query, {\em disproven} with counterexamples
that falsify the query, or {\em unknown} representing a failure respectively.
Given that 62.42\% of all queries are type II queries, support for counterexample generation for type II queries is found to be
extremely helpful in guiding the best-first search.
During over- and under-approximation attempts (for type II and type III queries), we use a strategy similar to ``iterative deepening'' \cite{iterativedeepening}.
Instead of using the maximum approximation depth ({\em eight} for our tests) first, we begin with a smaller value which is increased until
our attempt either succeeds or we reach the maximum permissible depth.
For prove (i.e. over-approximation) and disprove (i.e. under-approximation) attempts, we start with approximation depths {\em zero} and {\em two} respectively.
For both cases, we increase these depths by {\em one} until the maximum depth of {\em eight} is reached.
A lower value translates to a simpler SMT query leading to a higher probability of a faster solution and lower probability of timeouts by the off-the-shelf SMT solvers.
However, the precision of an attempt increases with higher approximation depths as previously discussed in detail in \cref{sec:approxdefs}.
\Cref{tab:querydepthdist} depicts the distribution of minimum approximation depths at which approximate (type II prove \& disprove and type III prove) queries
were successfully discharged (proven {\em or} disproven) by the solver.
It can be seen that the vast majority of the attempts were successful for approximation depths smaller than {\em eight}
and the number of successful attempts decreases exponentially with increasing depth.
The above observation reinforces the idea of ``iterative deepening'' because the time saved by solving smaller queries significantly
outweighs the comparatively rare loss of time in attempting the same query with increasing approximation depths.

The complete list of specifications used for our evaluation can be found in \cref{app:specifications}.
Additionally, \toolName{} along with the testcases used during evaluation is available as a docker image
at \cite{s2c_docker}.

% During the verification of {\tt strchr} and {\tt strpbrk} implementations,
% we identified an interesting subtlety. Since {\tt strchr} and {\tt strpbrk}
% return null pointers to signify absence of the required character(s) in the input string,
% we additionally need to model the UB assumption that the zero
% address does not belong to the null character terminated array representing the string.
% We use an explicit constructor \cons{SInvalid} to expose this well-formedness property in a \SpecL{} \type{String}.
% Furthermore, we relate \cons{SInvalid} to the condition of C character pointer being null using the
% lifting constructors \lifted{str}{\mem{}}{T}{p\ctype{i32},\dots} (as defined in \cref{tab:LiftingConsList}).
% These lifting constructors are used as part of \pre{} to equate $S$ and $C$ input strings.
% Finally in $S$, we model the absence of \cons{SInvalid} in the input string as a UB assumption using
% the {\tt assuming-do} statement introduced in \cref{sec:speclang}.
% Due to the \sdef{} assumption, this constraints the inputs to $S$ as well as $C$ to well-formed strings only.
% This is an example where \sdef{} and \pre{} can be used to model wellformedness of values in $C$.

\section{Limitations}
\label{sec:limitations}
\input{chapters/figures/figLimitationExStrchr.tex}
\toolName{} is not without limitations.
Since \toolName{} is only interested in finding a bisimulation relation,
a whole class of non-bisimilar but equivalent program pairs is beyond our scope.
In addition to \recursiveRelations{} based on the lifting constructors provided as part of \pre{} and \post{},
\toolName{} currently only supports bitvector affine and inequality relations.
Consequently, non-linear bitvector invariants (such as polynomial invariants) are not supported.
More importantly, \toolName{} does not attempt to infer lifting constructors and merely
uses the lifting constructors (with different arguments) provided as part of the input-output characteristics.
While our correlation and invariant inference algorithms are based on the Counter tool \cite{oopsla20},
which is designed primarily for equivalence checking between (C-like) unoptimized IR and assembly, we found them to be
quite effective for equivalence checking between \SpecL{} and deterministic C as well.
However, \toolName{} inherits many of the limitations of the Counter tool.

For example, \toolName{} fails to find a proof of equivalence if the unrolling
in the C implementation is higher than the unrolling parameter $\mu_S$ used during path
enumeration as part of the product-CFG construction algorithm.
Larger values of $\mu_S$ would significantly increase the correlation search space and likely
have negative implications on the runtime of the algorithm.
\input{chapters/figures/figLimitationExStrspn.tex}
Next, we demonstrate two common cases where \toolName{} is unable to find a proof of equivalence through two examples.
\Cref{fig:limitationExStrchr} shows our \SpecL{} specification and a C implementation for the C function {\tt strchr}.
Similarly, \cref{fig:limitationExStrspn} shows our \SpecL{} specification and a C implementation for {\tt strspn}.
Both specifications (in \cref{fig:limitationExStrchrSpec,fig:limitationExStrspnSpec}) are rather straightforward
and written to conform to the standard specifications of these functions.
The first C implementation (in \cref{fig:limitationExStrchrImpl}) defines {\tt strchr} in terms of another function {\tt strchrnul}.
\toolName{} fails to find a proof of equivalence in this case because it is unable to correlate the C function call edge at \cpc{3} with
a suitable function call in the specification. At present, our correlation algorithm only supports the correlation of a function call edge
with another function call edge.
\Cref{fig:limitationExStrspnImpl} shows an implementation of {\tt strspn} adapted from the musl C library implementation.
This implementation maintains a bitset (named `byteset') of $2^{8}$ bits to represent whether a character byte is present in the string `c'.
The bitset is initialized at \cpc{5} by {\em setting} those bits to 1 that correspond to characters contained in `c'.
Next, the input string `s' is traversed until it reaches the end or finds a character absent from the bitset.
Clearly, this implementation is drastically different from the naive algorithm of the specification (\cref{fig:limitationExStrspnSpec})
and non-bisimilar. Hence, \toolName{} fails in this case as well.
These two examples highlight two of the major drawbacks of \toolName{} -- (a) non-matching function calls and (b) non-bisimilar algorithmic choices.

Additionally, \toolName{} suffers from a similar trade-off with regards to the approximation parameters $d_o$ and $d_u$
-- a smaller than necessary value would cause a failed equivalence check while a larger value may
have major impact on the size of queries and counterexamples for reasons discussed next.
Consider a \recursiveRelation{} relating values of a non-linear ADT such as \type{Tree}.
It's $d$-depth approximation reduces into $\mathcal{O}(2^d)$ scalar equalities resulting in large SMT queries,
which to our experience is a major source of SMT solver timeouts during evaluation.
We partially subvert this issue by using an iterative deepening strategy for approximate queries described previously in \cref{sec:results}.

Also, the completeness of type III proof obligations is highly contingent on the precision
of our points-to analysis on \cprog{} as well as the deconstruction programs being
checked for equivanece as part of the nested bisimulation check.
We found our coarse-grained {\tt \{1,2+\}} categorization of allocation recency
combined with allocation-site based points-to analysis to be quite good at
identifying required points-to invariants.
However, such an abstraction is far from complete.