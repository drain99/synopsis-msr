\chapter{Spec-to-C Equivalence Checker}
\label{sec:spectocalgo}
This chapter presents our automatic equivalence checker algorithm \toolName{}.
Given a \SpecL{} and a C program along with the input-output specification for each function-pair,
\toolName{} searches for a proof of equivalence between the CFGs of each \SpecL{} and C procedures: \sprog{} and \cprog{}.
Recall that CFGs represent deterministic programs and evidently, the C procedure is determinized during conversion to \cprog{}.
Hence, \toolName{} checks equivalence between a \SpecL{} procedure and the determinized C procedure.
A translation validator (such as the Counter tool \cite{oopsla20}) can be used to check equivalence between
the same determinized C procedure against its generated assembly.
By restricting the deterministic choices made by the C compiler to the same choices made during construction of \cprog{},
we can effectively establish end-to-end equivalence between a \SpecL{} procedure against its assembly.
We start with a dataflow formulation of the points-to analysis used as part of \toolName{} on \cprog{} as well as
deconstruction programs during discharge of type III proof obligations in \cref{sec:pointsToFormal}.
As stated in \cref{sec:contribs}, \toolName{} is based on three primary interdependent algortihms:
\curved{A1} an algorithm to incrementally construct a product-CFG by correlating program executions across
\sprog{} and \cprog{},
\curved{A2} an algorithm to identify inductive invariants at intermediate PCs in the (partially constructed)
product-CFG, and
\curved{A3} an algorithm for solving proof obligations generated by \curved{A1} and \curved{A2} algorithms.
We describe our counterexample-guided best-first search algorithm for construction of a product-CFG (\curved{A1}) in \cref{sec:searchalgo}.
This is followed by a dataflow formulation of our counterexample-guided invariant inference algorithm (\curved{A2}) in \cref{sec:invinferalgo}.
In the last chapter, we walked through our proof discharge algorithm (\curved{A3}) using examples, ending with
the pseudo-code of proof discharge algorithm (\cref{algo:proofSummary} in \cref{sec:proofsummary}).
In this chapter, we present pseudo-codes of the major subprocedures used as part of
the proof discharge algorithm\footnote{TODO:keep the full proof discharge algo here or keep it at the end of examples where it is now?}.

\input{chapters/figures/figPointstoDFA.tex}

\section{Points-to Analysis}
\label{sec:pointsToFormal}
Recall that in \cref{sec:reconsbisim}, we needed to reason about aliasing to successfully discharge a type III proof obligation.
These aliasing relationships are described in \cref{sec:pointsTo} and used in \cref{sec:pointsToAsInvariants} to
successfully discharge a type III proof obligation.
A points-to analysis is used to identify these relationships in \cprog{} as well as each deconstruction program \dprog{}.
\Cref{tab:pointstoalgodfa} presents a dataflow formulation of our points-to analysis.
We start by identifying the set \memregions{} of all region labels representing mutually non-overlapping
regions of the \cprog{} memory \mem{}.
For each call to {\tt malloc()} at PC $A$, we add $A_1$ and $A_{2+}$ to \memregions{}.
Recall that $A_1$ represents the region of memory returned by the {\em most recent} execution of $A$.
$A_{2+}$ represents the region of memory returned by older (i.e. all but most recent) executions of $A$.
$\memregions{} = \bigcup_{A} \{ A_1, A_{2+} \} \cup \{ \heapr{} \}$,
where \heapr{} is the region of memory \mem{} not covered by the labels associated with allocation sites.
Note that \memregions{} is computed globally i.e. in case $C$ consists of multiple procedures,
\memregions{} is identical for each and contains regions associated with allocation sites of {\tt malloc()}
calls for all procedures.

Let $\pseudoregs{}^{\cprog{}}$  be the set of all scalar pseudo-registers in \cprog{}.
We use a forward dataflow analysis to identify a may-point-to function
$\Delta^{\cprog{}}: (\pseudoregs{}^{\cprog{}} \cup \memregions{}) \mapsto 2^{\memregions{}}$ at each program point in \cprog{}.
For a deconstruction program \dprog{}, we are also interested in finding the may-point-to function for
all scalar pseudo-registers in \dprog{}, say $\pseudoregs{}^{\dprog{}}$.
Thus, the domain of the may-point-to function for \dprog{} ($\Delta^{\dprog{}}$) contains $\pseudoregs{}^{\dprog{}}$
in addition to the domain of $\Delta^{\cprog{}}$ i.e.
$\Delta^{\dprog{}}: (\pseudoregs{}^{\cprog{}} \cup \memregions{} \cup \pseudoregs{}^{\dprog{}}) \mapsto 2^{\memregions{}}$.
The '$\pointsTo$' operator introduced in \cref{sec:pointsTo} is called the {\em element-wise} may-point-to function
and is related to the may-point-to function $\Delta$ as follows: $p \pointsTo{} S \Leftrightarrow \Delta(p) = S$.

The meet operator is element-wise set-union e.g., $p \pointsTo{} S_1$ and $p \pointsTo{} S_2$
combines into $p \pointsTo{} S_1 \cup S_2$.
Evidently, the $\top$ value is the constant function that returns $\emptyset$.
At entry of \cprog{}, we conservatively assume that all memory regions may point to each other.
However, at entry of a deconstruction program \dprog{}, created during a proof obligation at product-CFG node $(n_S\!:\!n_C)$,
we use \cprog{}'s precomputed may-point-to function at $n_C$ ($\Delta^{\cprog{}}_{n_C}$)
to initialize the points-to relationships for all state elements of \cprog{} (i.e. $(\pseudoregs{}^{\cprog{}} \cup \memregions{})$).
This is a crucial step for proving equality of \cprog{} values under different memory states as seen in \cref{sec:pointsToAsInvariants}.

Next, we discuss the transfer function $f_e$ for our points-to analysis.
For an IR instruction ${\tt x} \coloneqq {\tt c}$, for constant {\tt c}, the
transfer function updates $\Delta({\tt x}) \coloneqq \emptyset$.
For instruction ${\tt x} \coloneqq {\tt y\ op\ z}$ (for some arithmetic or logical operator {\tt op}),
we update $\Delta({\tt x}) \coloneqq \Delta({\tt y}) \cup \Delta({\tt z})$.
For a load instruction ${\tt x} \coloneqq \memRead{\mem{}}{y}{T}$, we
update $\Delta({\tt x}) \coloneqq \bigcup_{t \in \Delta(y)} \Delta(t)$.
For a store instruction $\mem{} \coloneqq \memWrite{\mem{}}{x}{y}{T}$, for all
$t \in \Delta({\tt x})$, we update $\Delta(t) \coloneqq \Delta(t) \cup \Delta(y)$.
For a malloc instruction ${\tt x} \coloneqq {\tt malloc}_A()$
(where $A$ represents the allocation site), we perform the following steps (in order):
\begin{enumerate}
\item Convert all existing occurrences of $A_1$ to $A_{2+}$, i.e., for all $t \in (\pseudoregs{}^{\cprog{}} \cup \memregions{})$,
if $A_1 \in \Delta(t)$, then update $\Delta(t) \coloneqq (\Delta(t) \setminus \{ A_1 \}) \cup \{ A_{2+} \}$.
\item Update $\Delta({\tt x}) \coloneqq \{ A_1 \}$.
\item Update $\Delta(A_{2+}) \coloneqq \Delta(A_{2+}) \cup \Delta(A_1)$.
\item Update $\Delta(A_1) \coloneqq \emptyset$.
\end{enumerate}

For function calls, a {\em supergraph} is created by adding control flow edges
from the call-site to the procedure head (copying actual arguments to the formal arguments) and
from the procedure exit to the program point just after the
call-site (copying returned value to the variable assigned at the callsite),
e.g., in \cref{fig:clistdeconsCFG}, the dashed edges represent supergraph edges.

The allocation-site abstraction (with a bounded-depth call stack) is
known to be effective at disambiguating memory regions belonging to
different data structures
\cite{allocationSiteAbstraction82,allocationSiteAbstraction90,allocationSiteAbstraction06}.
In our work, we also need to reason about non-aliasing
of the most-recently allocated object (through a {\tt malloc} call) and
the previously-allocated objects (as in the \type{List}
construction example). The coarse-grained $\{1, 2+\}$
categorization of allocation recency is effective for such disambiguation.

\input{chapters/figures/figSearch.tex}

\section{Counterexample-guided Product-CFG Construction}
\label{sec:searchalgo}
\toolName{} constructs a product-CFG incrementally to search for a bisimulation relation
between the \SpecL{} and C CFGs : \sprog{} and \cprog{}.
Multiple candidate product-CFGs are partially constructed during this search;
the search completes when one of these candidates yield an equivalence proof.

{\em Anchor nodes} are identified in \sprog{} and \cprog{}, and represents the
CFG nodes (i.e. IR PCs) being considered for correlation.
The algorithm ensures that every cycle in both \sprog{} and \cprog{} contains at least one anchor node.
The start and exit nodes are always anchor nodes.
Also, for every function call, the nodes just before and after its callsite are considered anchor nodes.
For example, in \cref{fig:llAllocCCFG}, \cpc{4} and \cpc{5} are anchor nodes around the call to {\tt malloc}.
The selected anchor nodes for the CFGs in \cref{fig:llAllocSpecIRCFG,fig:llAllocCCFG} are:
$\{ \spc{0},\spc{3},\spc{E} \}$ and $\{ \cpc{0}, \cpc{3}, \cpc{4}, \cpc{5}, \cpc{E} \}$ respectively.
For each anchor node in \cprog{}, our search algorithm searches for a correlated anchor node in \sprog{} --- if
a (partially constructed) product-CFG $\pi$ contains a product-CFG node  $(n_S\!:\!n_C)$, then $\pi$
correlates node $n_C$ in \cprog{} with node $n_S$ in \sprog{}.
The search procedure begins with a single partially-constructed product-CFG $\pi_{init}$.
$\pi_{init}$ contains exactly one node (\scpc{0}{0}) that encodes the correlation of the entry nodes
(i.e. \spc{0} and \cpc{0}) of \sprog{} and \cprog{}.

\subsection{Correlating Pathsets}
\label{sec:pathsetcorrel}
At each step of the incremental construction process, a node $(n_S\!:\!n_C)$ is chosen in a product-CFG $\pi$
and a 1-{\em pathset} $\xi_C$ in \cprog{} starting at $n_C$ (and ending at an anchor node in \cprog{}) is selected.
Then, we enumerate potentially correlated $\mu$-{\em pathsets} in \sprog{} for the pathset $\xi_C$ in \cprog{}.
A {\em pathset} $\xi$ is essentially a set of paths with the following additional properties:
(a) all paths $\rho \in \xi$ begin at the same node and terminate at the same node,
(b) all paths $\rho \in \xi$ are mutually-exclusive i.e. {\em at most} one of ${\tt pathcond}_\rho$ can be true.
A $\mu$-pathset $\xi$ is a pathset where each path $\rho \in \xi$ contains {\em at most} $\mu$ occurrences of any node.
$\mu$-pathsets are based on earlier work on the Counter tool \cite{oopsla20}\footnote{
Counter tool \cite{oopsla20} considers $(\mu,\delta)$-unrolled pathsets in its correlation search algorithm.
Our $\mu$-pathsets are a special case of $(\mu,\delta)$-unrolled pathsets where $\delta = \mu$.}
and helps improve the completeness of the bisimulation search
by attempting to correlate a set of \cprog{} paths with a set of \sprog{} paths,
where individual paths of \cprog{} and \sprog{} may be uncorrelated.
An example of such a scenario is depicted in \cref{sec:expstring}.
\Cref{sec:pathsethoaretriples} briefly discusses the techniques of handling Hoare triples (i.e. proof obligations)
involving pathsets.

Revisiting our incremental construction process, recall that we choose a 1-pathset $\xi_C$ in \cprog{}
and enumerate potentially correlated $\mu$-pathsets $\xi_S$ in \sprog{}.
Hence, $\mu$ represents the maximum number of iterations of a loop (in \sprog{}),
that may be correlated with a pathset in \cprog{} consisting of acyclic paths.
$\mu$ is a fixed parameter of the \toolName{} algorithm and is called the {\em unroll factor}.
For example, during construction of the product-CFG shown in \cref{fig:llAllocProductCFG},
say we select the product-CFG node (\scpc{3}{3}).
We choose the \cprog{} path(set) \cpath{3,4} and enumerate its potential correlations (i.e. $\mu$-pathsets in \sprog{} starting at \spc{3}):
$\epsilon$, \spath{3,5,3}, \ldots, $\pathset{S3,(\pathset{S5,S3})^{(\mu-1)}}$.
Importantly, for pathsets $\xi_S$ (in \sprog{}) and $\xi_C$ (in \cprog{}) to be considered for correlation,
they must originate and terminate at anchor nodes, i.e. the path \spath{3,5} is skipped during enumeration.
Moreover, the pathset $\xi_C$ may only contain anchor nodes as its source and destination.
Hence, the path \cpath{3,4,5} is not considered for $\xi_C$,
instead we attempt to correlate the subpaths \cpath{3,4} and \cpath{4,5} individually.

For each enumerated correlation possibility $(\xi_S,\xi_C)$, a separate product-CFG $\pi'$ is
created (by cloning $\pi$) and a new product-CFG edge $e=(\xi_S,\xi_C)$ is added to $\pi'$.
The head of the product-CFG edge $e$ is the (potentially newly added) product-CFG node representing
the correlation of the end-points of pathsets $\xi_S$ and $\xi_C$. For example, the node (\scpc{3}{4}) is added
to the product-CFG if it correlates pathsets $\epsilon$ and \cpath{3,4} starting at (\scpc{3}{3}).
Recall that, we consider $\epsilon$ as a candidate for $\xi_S$, but not $\xi_C$.
The algorithm ensures that no cycle in \cprog{} is correlated with $\epsilon$ in \sprog{}
(to preserve divergence discussed in \cref{sec:bisim}).
For each node $n$ in a product-CFG $\pi$, we maintain a small number of
concrete machine state pairs (of \sprog{} and \cprog{}).
The concrete machine state pairs at $n$ are obtained as counterexamples to an unsucessful proof
obligation \hoareTriple{\phi_s}{s \rightarrow d}{\phi_d} (for some edge $s \rightarrow d$ and node $d$ in $\pi$).
Thus, by construction, these counterexamples represent concrete state pairs that may potentially occur
at $n$ during the lockstep execution encoded by $\pi$.

\subsection{Best-First Ranking of Partial Product-CFGs}
\label{sec:rankproductcfg}
To evaluate the promise of a possible correlation $(\xi_S,\xi_C)$ starting at node $n$
in product-CFG $\pi$, we examine the execution behaviour of the counterexamples at $n$ on
the product-CFG edge $e=(s\rightarrow d)=(\xi_S,\xi_C)$.
If the counterexamples ensure that the machine states remain related at $d$,
then that candidate correlation is ranked higher.
This ranking criterion is based on prior work \cite{oopsla20}.
A best-first search (BFS) procedure based on this ranking criterion is used to incrementally construct
a product-CFG (starting from $\pi_{init}$).
For each intermediate candidate product-CFG $\pi$ generated during this search procedure,
an automatic invariant inference procedure (discussed next in \cref{sec:invinferalgo}) is
used to identify invariants at all the nodes in $\pi$.
The counterexamples obtained from the proof obligations generated by this invariant inference
procedure are added to the respective nodes in $\pi$; these counterexamples help rank
future correlations starting at those nodes.

If after invariant inference, we realize that an intermediate candidate product-CFG $\pi_1$
is not promising enough, we backtrack and choose another candidate product-CFG $\pi_2$
and explore the potential correlations that can be added to $\pi_2$.
Thus, a product-CFG is constructed one edge at a time.
If at any stage, a product-CFG $\pi$ contains correlations for every path in \cprog{}
and invariants ensure equal observables (i.e. \post{} holds at correlated exit nodes),
we have successfully shown equivalence.
This counterexample-guided BFS procedure is similar to the one described in prior work on
the Counter algorithm \cite{oopsla20}.

\subsection{Correlation in the Presence of Function Calls}
\label{sec:correlfcalls}
Recall that \sprog{} and \cprog{} may make function calls (including self calls),
e.g., allocation of memory in C, recursive traversal of a tree data structure.
Recall that the nodes just before and after a function call are always considered anchor nodes.
Calls to memory allocation functions in \cprog{} (i.e. {\tt malloc}) are handled by correlating
the function call edge with the empty path ($\epsilon$) in \sprog{}.
For example, in the product-CFG shown in \cref{fig:llAllocProductCFG}, the {\tt malloc} edge \cpath{4,5} in \cprog{}
is correlated with $\epsilon$ in \sprog{}.

For all other calls, our correlation algorithm (in \cref{sec:searchalgo}) ensures that the anchor nodes
around such a callsite are correlated one-to-one across both procedures.
For example, let there be a call to procedure $\delta$ in \sprog{} at PC $n_S$, i.e. $n_S$ is the call-site.
Let us denote the program point just after this call-site as $n'_S$.
Let {\tt args}$_{n_S}$ represent the values of the actual arguments of this function call (at $n_S$).
Let {\tt ret}$_{n'_S}$ represent the value returned by this function call (at $n'_S$).
Similarly, for a procedure call $\delta$ in \cprog{}, let $n_C$, $n'_C$, {\tt args}$_{n_C}$ and {\tt ret}$_{n'_C}$
represent the function call-site, program point just after the call-site,
the values of the actual arguments and the value returned respectively.
Our algorithm ensures that the only correlation possible in a product-CFG $\pi$ for these program points are
$(n_S:n_C)$ and $(n'_S:n'_C)$.

We utilize the user-supplied input-output specification for $\delta$ (say $(\pre{}_{\delta},\post{}_{\delta})$)
to obtain the desired invariants at nodes $(n_S:n_C)$ and $(n'_S:n'_C)$ in the product-CFG.
A successful proof must {\em ensure} that $Pre_{\delta}$({\tt args}$_{n_S}$,{\tt args}$_{n_C}$) holds at $(n_S:n_C)$.
Further, the proof can {\em assume} that $Post_{\delta}$({\tt ret}$_{n'_S}$,{\tt ret}$_{n'_C}$) holds at $(n'_S:n'_C)$.
Note that {\tt args}$_{n_C}$ and {\tt ret}$_{n'_C}$ includes the \cprog{} memory states
$\mem{}_{n_C}$ (at $n_C$) and $\mem{}_{n'_C}$ (at $n'_C$) respectively.
Thus, for function calls, we inductively prove the precondition (on the arguments) at $(n_S:n_C)$
and assume the postcondition (on the returned values) at $(n'_S:n'_C)$.

\input{chapters/figures/figInvariantDFA.tex}
\input{chapters/figures/figTfAndInvGrammar.tex}

\section{Invariant Inference and Counterexample Generation}
\label{sec:invinferalgo}
We formulate our counterexample-guided invariant inference algorithm as a dataflow analysis
as shown in \cref{tab:invinferalgodfa}.
The invariant inference procedure is responsible for inferring invariants $\phi_n$ at each intermediate
node $n$ of a (partially constructed) product-CFG, while also generating a set of counterexamples
$\Gamma_n$ that represents the potential concrete machine states at $n$.

Given the invariants and counterexamples at node $s$: ($\phi_s,\Gamma_s$),
the transfer function initializes the new candidate set of counterexamples at $d$ ($\Gamma^{can}_{d}$)
with the current set of counterexamples at $d$ ($\Gamma_{d}$) {\em union}-ed with
the counterexamples obtained by executing $\Gamma_s$ on edge $e$ (through {\tt exec}$_e$).
The candidate invariant at $d$ ($\phi^{can}_d$) is computed as the strongest cover
of $\Gamma^{can}_{d}$ ($StrongestInvCover()$).
At each step, the transfer function attempts to prove $\{\phi_s\} (e) \{\phi^{can}_d\}$
(through a call to $Prove()$).
If the proof succeeds ($Prove()$ returns \cons{True}), the candidate invariant $\phi^{can}_d$ is returned along with
the counterexamples $\Gamma^{can}_d$ learned so far.
Otherwise, $Prove()$ returns $\cons{False}(\gamma_s)$.
The candidate invariant $\phi^{can}_d$ is weakened using the counterexamples obtained
(i.e. $\gamma_s$) and the proof attempt is repeated.

The candidate invariants are drawn from the predicate grammar \invgrammar{} shown in \cref{fig:invinfergrammar}.
In addition to affine and inequality relations between bitvectors in \sprog{} and \cprog{},
\invgrammar{} supports \recursiveRelations{} between an ADT variable in \sprog{} and a lifted expression in \cprog{}.
The candidate lifting constructors of the form \lift{lift}{\mem{}}{T} (where \mem{} is the current
memory state in \cprog{}) are derived from the lifting constructors
present in the precondition \pre{} and the postcondition \post{}, as supplied by the user.
More sophisticated strategies for inference of new lifting constructors is left as future work.

$StrongestInvCover()$ for affine relations involve
identifying the basis vectors of the kernel of the
matrix formed by the counterexamples in the bitvector
domain \cite{esop05,semalign}.
For inequality relations, $StrongestInvCover(\Gamma)$
returns {\em true} (i.e. the weakest invariant) iff any counterexample in $\Gamma$ evaluates the
relation to false --- this effectively simulates the Houdini approach \cite{houdini}.
Similarly, in case of a \recursiveRelation{} $l_1 \indEq{} l_2$, $StrongestInvCover(\Gamma)$
returns {\em true} iff any counterexample in $\Gamma$ evalutes its $\eta$-depth over-approximation
$l_1 \indEqDepth{\eta} l_2$ to false (effectively falsifying a weaker condition), where $\eta$ is a fixed parameter of the algorithm.

\section{More on Proof Discharge Algorithm}
\label{sec:proofalgo}
TODO: write 1-2 lines about what "more" information this section contains

\subsection{Handling Proof Obligations On Pathsets}
\label{sec:pathsethoaretriples}
Recall that our correlation algorithm attempts to correlate pathsets (instead of paths) between \sprog{} and \cprog{}.
Evidently, each edge of a (possibly partial) product-CFG $\pi$ is associated with a pair of pathsets $(\xi_S,\xi_C)$.
Proof obligations originating across a product-CFG edge $e[s \rightarrow d]=(\xi_S,\xi_C)$ are of
the form \hoareTriple{\phi_s}{\xi_S,\xi_C}{\phi_d}.
A Hoare triple of the above form can be broken down into a conjunction of Hoare triples involving purely paths as follows:

\begin{equation}
\label{eqn:hoaretriplepathset}
\hoareTriple{\phi_s}{\xi_S,\xi_C}{\phi_d} \Leftrightarrow \bigwedge_{\substack{\rho_S \in \xi_S \\ \rho_C \in \xi_C}} \hoareTriple{\phi_s}{\rho_S,\rho_C}{\phi_d}
\end{equation}

Recall that our proof discharge algorithm requires that proof obligations satisfy the conjunctive \recursiveRelation{} property (defined in \cref{sec:proofalgoprops}).
If the original proof obligation \hoareTriple{\phi_s}{\xi_S,\xi_C}{\phi_d} satisfies this property, so does each of the smaller
proof obligation \hoareTriple{\phi_s}{\rho_S,\rho_C}{\phi_d}.
Hence, the proof discharge algorithm presented in \cref{sec:examples} is capable of handling these smaller proof obligations
and by \cref{eqn:hoaretriplepathset}, also the original proof obligations.
If $|\xi|$ represents the number of paths in a pathset $\xi$, the proof obligation \hoareTriple{\phi_s}{\xi_S,\xi_C}{\phi_d} results
in $|\xi_S| \times |\xi_C|$ smaller proof obligations.
In practice, the number of paths in \sprog{} that are correlated with a path in \cprog{} is quite low
and consequently, most of these proof obligations usually end up with a {\tt false} \lhs{} when lowered to
first-order logic (through \cref{eqn:firstOrderFormula}), due to the presence of {\tt pathcond}$_{\rho_S}$ and {\tt pathcond}$_{\rho_C}$.
Our proof discharge procedure begins with an attempt to disprove \lhs{} (overapproximated in case of \recursiveRelations{}),
which trivially resolves the smaller proof obligations involving uncorrelated paths to {\tt true}.

\subsection{Canonicalization Procedure}
\label{sec:canonicalalgo}

\input{chapters/figures/figCanonical.tex}

\Cref{algo:canonical} shows the pseudo-code for the canonicalization procedure.
$Canonicalize(e)$ is responsible for converting an expression $e$ to its canonical form $\hat{e}$ (introduced in \cref{sec:unifyandrewrite}).
Recall that a pseudo-variable is an expression of the form $\prodAccess{v}{a_1,a_2,.,a_n}$, where $v$ is a variable.
Also recall that, an expression $e$ is canonical iff each {\em accessor} and {\em sum-is} expression operate on a pseudo-variable.
An ADT expression with a data constructor, a lifting constructor or the \sumDtor{}-operator at its top-level, is called a {\em foldable} expression.
$Canonicalize(e)$ iteratively folds each {\em accessor} and {\em sum-is} subexpressions of $e$ that operate on a foldable argument.
Thus, $Canonicalize(e)$ returns an expression where none of the {\em accessor} or {\em sum-is} subexpressions is foldable.
This condition entails the requirements of the canonical form.
For example, $a + \prodAccess{\cons{LCons}(b,l)}{tail,val}$ and $\sumIs{\lifted{list}{\mem{}}{lnode}{p}}{LNil}$
canonicalizes to $a + \prodAccess{l}{val}$ and $(p = 0)$ respectively.

\subsection{Unification Procedure}
\label{sec:unifalgo}

\input{chapters/figures/figUnification.tex}

\Cref{algo:unification} shows the pseudo-code for the unification algorithm introduced in \cref{sec:unifyandrewrite}.
$\theta(p_1,e_1,p_2,e_2)$ is responsible for unifying expressions $e_1$ and $e_2$ under the expression path
conditions $p_1$ and $p_2$ respectively.
$\theta$ either fails to unify with the \cons{Fail} output, or it successfully returns $\cons{Succ}(S)$, where $S$
is the set of correlation tuples that relate (a) either two atomic expressions, or (b) an atom with an non-atomic expression.
$\theta(p_1,e_1,p_2,e_2)$ terminates when one of $e_1$ and $e_2$ is an atomic expression.
In case both $e_1$ and $e_2$ contains a data constructor at their top-level, 
$\theta$ attempts to recursively unify the data constructors and their corresponding children.
If exactly one of $e_1$ and $e_2$ is a \sumDtor{} expression,
$\theta$ attempts to unify both branches of \sumDtor{} (along with the path conditions) with the other expression
and return whichever succeeds.
If both $e_1$ and $e_2$ are \sumDtor{} expressions, $\theta$ attempts to recursively unify their children.
$\theta$ uses the $\sqcup$ -operator to combine the results of successive self-calls.
$A \sqcup B$ is equal to $\cons{Succ}(S_1 \cup S_2)$ if $A = \cons{Succ}(S_1)$ and $B = \cons{Succ}(S_2)$;
otherwise (if one of $A$ and $B$ is \cons{Fail}), $A \sqcup B = \cons{Fail}$.
Additionally, for a \sumDtor{} expression with \underline{\tt if} condition $c$, $c$ is well-formed under the expression path condition.
Hence, when conjuncting $c$ to the expression path condition, we use an `ordered and' operator \oland;
$e_1 \oland e_2$ is equivalent to $e_1 \land (e_1 \rightarrow e_2)$.

\subsection{Iterative Unification and Rewriting Procedure}
\label{sec:unifyandrewritealgo}

\input{chapters/figures/figIterUnifyAndRewrite.tex}

\Cref{algo:unifyandrewrite} shows the pseudo-code for the iterative unification and rewriting procedure
introduced in \cref{sec:unifyandrewrite}.
$\Theta(p_a,e_a,p_b,e_b)$ is responsible for unifying expressions $e_a$ and $e_b$ under the expression
path conditions $p_a$ and $p_b$ respectively.
$\Theta$ either fails to unify with the \cons{Fail} output, or it successfully returns $\cons{Succ}(S)$, where $S$
is the set of correlation tuples that relate {\em only} atomic expressions.
$\Theta$ attempts to iteratively (a) unify the expressions (through a call to the unification procedure $\theta$ in \cref{sec:proofalgo}),
and (b) perform rewriting (of atom $a_1$ for those correlation tuples \corrtuple{p_1}{a_1}{p_2}{e_2} where $e_2$ is non-atomic), followed by
a recursive call to $\Theta$. TODO recall example from \cref{sec:decomprecrel}

\subsection{Decomposition Procedure for Recursive Relations}
\label{sec:decomposealgo}

\input{chapters/figures/figDecompose.tex}

\Cref{algo:decompose} shows the pseudo-code for the decomposition algorithm defined in \cref{sec:unifyandrewrite}.
$Decompose(l_1, l_2)$ is responsible for computing the decomposition of the \recursiveRelation{} $l_1 \indEq{} l_2$.
Recall that, decomposition of a \recursiveRelation{} $l_1 \indEq{} l_2$ requires the unification of (canonicalized)
$l_1$ and $l_2$ through the top-level invocation of $\Theta(true,l_2,true,l_2)$.
If the $n$ correlation tuples obtained after a successful unification are \corrtuple{p_1^i}{a_1^i}{p_2^i}{a_2^i}
(for $i=1\ldots n$), then the decomposition of $l_1 \indEq{} l_2$ is defined by \cref{eqn:decompose}.
If the unification fails (with a \cons{Fail} output), the decomposition is defined to be {\tt false}.
TODO recall example from \cref{sec:decomprecrel}

\subsection{Reduction Procedures for Approximate Recursive Relations}
\label{sec:approxalgo}
Recall that type II proof obligations (summarized in \cref{sec:cat2summary}) are discharged
by over- and under-approximating the \lhs{} (resulting in a weaker and a stronger proof obligation respectively),
followed by discharging both proof obligations through SMT solvers.
We overapproximate \lhs{} by substituting each \recursiveRelation{} $l_1 \indEq{} l_2$ in the \lhs{}
with its $d_o$-depth overapproximation $l_1 \indEqDepth{d_o} l_2$.
Similarly, the \lhs{} is underapproximated by substituting each \recursiveRelation{} $l_1 \indEq{} l_2$
with its $d_u$-depth underapproximation $l_1 \indEqDepth{d_u} l_2$.

\subsubsection{$D$-depth Iterative Unification and Rewriting Procedure}
\label{sec:dunifyandrewritealgo}

\input{chapters/figures/figDUnification.tex}
\input{chapters/figures/figDIterUnifyAndRewrite.tex}

Recall that, \cref{sec:approxdecomp} briefly describes the process of reducing an
overapproximate \recursiveRelation{} into its SMT-encodable equivalent absent of \recursiveRelations{}.
We use modified versions of unification and `iterative unification and rewriting' procedures
(defined in \cref{sec:unifalgo,sec:unifyandrewritealgo} respectively) to
reduce an overapproximate \recursiveRelation{} into its SMT-equivalent.
The $D$-depth unification and `iterative unification and rewriting' procedures
are represented by $\Theta_D(p_a,e_a,p_b,e_b,d)$ and $\theta_D(p_1,e_1,p_2,e_2,d)$
respectively, where $D$ is a parameter of the algorithm.
The pseudo-code for these two procedures are shown in \cref{algo:dunification,algo:dunifyandrewrite} respectively.
The $D$-depth `iterative unification and rewriting' returns depth-augmented
correlation tuples of the form $\corrtuple{p_1}{a_1}{p_2}{a_2}_d$ such that $d \geq D$
for all correlation tuples.
Unlike $\Theta$ which terminates unification iff both expressions are atomic,
$\Theta_D$ performs rewriting of both ADT atomic expressions and continues to
unify deeper into their respective expression trees until all correlation tuples relate
expressions at depth $\geq D$. TODO recall example from \cref{sec:approxdecomp}

\subsubsection{Reduction Procedure for Overapproximate Recursive Relations}
\label{sec:overapproxalgo}

\input{chapters/figures/figOverapprox.tex}

$Overapprox_D(l_1,l_2)$ is responsible for reducing the overapproximation $l_1 \indEqDepth{D} l_2$
into its SMT-encodable equivalent condition.
$Overapprox_D$ is similar to the decomposition procedure in \cref{sec:decomposealgo} except it only preserves
scalar equalities till a maximum depth of $D$ (inclusive).
This essentially asserts that both $l_1$ and $l_2$ have identical structures (by equating expression path conditions)
and equal scalar values (by equating scalar leaf expressions) till a depth of $D$.
TODO recall example from \cref{sec:approxdecomp}

\subsubsection{Reduction Procedure for Underapproximate Recursive Relations}
\label{sec:underapproxalgo}

\input{chapters/figures/figDepthBounded.tex}

Recall that, an underapproximate \recursiveRelation{} $l_1 \indEqUapprox{d_u} l_2$
is equivalent to $\depthBound{d_u}{l_1} \land \depthBound{d_u}{l_2} \land l_1 \indEqDepth{d_u} l_2$,
where $\depthBound{d}{l}$ asserts that $l$ has a depth of at most $d$ (defined in \cref{sec:approxdefs}).
The function responsible for computing $\depthBound{D}{l}$ is $isDepthBounded_D(l,p,d)$,
where $p$ and $d$ represents the current expression path condition and depth of $l$,
and $D$ is a parameter of the algorithm.
\Cref{algo:depthbound} gives the pseudo-code for $isDepthBounded_D$.
The top-level invocation is given by $isDepthBounded_D(l,true,0)$.
$isDepthBounded_D$ recursively traverses the expression tree of $l$ (while rewriting as necessary),
until it reaches a node at depth $>D$, at which point it returns the condition asserting the unreachability
of such a node. TODO recall example from \cref{sec:approxdecomp}

\input{chapters/figures/figUnderapprox.tex}

Finally, $Underapprox_D(l_1,l_2)$ is responsible for reducing the underapproximation $l_1 \indEqUapprox{D} l_2$
into its SMT-encodable equivalent condition.
The pseudo-code for $Underapprox_D$ is given in \cref{algo:underapprox}.
TODO recall example from \cref{sec:approxdecomp}

\subsection{SMT Encoding of First Order Logic Formula}
\label{sec:smtencoding}
As summarized in \cref{algo:proofSummary}, our proof discharge algorithm solves a proof obligation $P: \lhs{} \Rightarrow \rhs{}$,
through a sequence of queries $P_i : \lhs{}_i \Rightarrow \rhs{}_i$ to off-the-shelf SMT solvers.
Recall that $P$ may contain \recursiveRelations{}.
However, our algorithm ensures that each $P_i$ is free of \recursiveRelations{} and only contain
scalar equalities.
We encode each query $P_i$ in SMT logic with bitvector and array theories.
In this section, we describe the process of encoding a proof obligation $P_i$ into SMT logic.
We begin by converting $P_i : \lhs{}_i \Rightarrow \rhs{}_i$ into its canonical form $\hat{P}_i$
(as described in \cref{sec:canonicalalgo}).
Although $\hat{P}_i$ does not contain \recursiveRelations{}, it may still contain
ADT variables alongside {\em accessor} and {\em sum-is} expressions.
Due to canonicalization, all top-level {\em accessor} and {\em sum-is} expressions must be of the form
$\prodAccess{v}{a_1,a_2,.,a_n}$ and $\sumIs{\prodAccess{v}{a_1,a_2,.,a_n}}{\textnormal{V}}$ respectively.
We call such an expression $e$ {\em flattenable} and the ADT variable $v$ is called the {\em index} of $e$.
$\hat{P}_i$ is lowered into an intermediate expression $P_i^f$ through a process called {\em flattening}.
This involves `flattening' all flattenable expressions to variables such that
$P_i^f$ only contains scalar values with scalar and memory operations (but importantly not ADT values).
The flattening process is described below.

\begin{enumerate}
\item For each top-level {\em accessor} expression $e = \prodAccess{v}{a_1,a_2,.,a_n}$, we replace it with a
variable named $v \strcat{} \field{a_1} \strcat{} \field{a_2} \strcat{} \dots \strcat{} \field{a_n}$,
where \strcat{} concatenates two strings with a `\strsep{}' character in between i.e.
$"a" \strcat{} "b" = "a \strsep{} b"$.

\item For each ADT $T$ with data constructors $V_1,V_2,\dots,V_k$,
we define an enumeration type $\mathcal{E}(T)$ in SMT logic with items
$\mathcal{E}(V_1),\mathcal{E}(V_2),\dots,\mathcal{E}(V_k)$ respectively.
In the canonical form, each {\em sum-is} expression $e$ must operate on a pseudo-variable.
The last step guarantees that $e$ must be of the form: $e = \sumIs{v}{\textnormal{V}}$.
We replace $e$ with the its SMT equivalent: $(v \strcat{} tag) = \mathcal{E}(V)$
\footnote{\SpecL{} does not allow naming a field of a data constructor \field{tag}.
Furthermore, fields cannot contain the `\_' character.
Combined, these two conditions prevent collision between variable names obtained due to flattening.}.
\end{enumerate}

For example, the canonical expression $a + \prodAccess{l}{val}$ flattens to $a + l \strsep{} val$.
Similarly, $(\sumIs{\prodAccess{l}{tail}}{LCons})$ flattens to $l \strsep{} tail \strsep{} tag = \mathcal{E}(\cons{LCons})$.
Due to flattening, each flattenable expression $e$ in $\hat{P}_i$ with index $v$ gets lowered
into a variable in $P_i^f$ whose name begins with $v \strsep$.
For the ADT variable $v$, let $\mathcal{F}(v)$ be the set of all such lowered variables in $P_i^f$.
For example, flattening of an expression with $\prodAccess{l}{val}$ and $\sumIs{l}{LCons}$
results in $\mathcal{F}(l) = \{ l \strsep{} val, l \strsep tag \}$.
Importantly, $P_i^j$ may only contain scalar and memory operations (but not ADT values).

Scalar types and their operations map one-to-one to their SMT equivalents.
The memory element \mem{} is represented as a byte-addressable (i.e. \type{i8}) array.
A memory load \memRead{\mem{}}{a}{T} is expanded into the concatenation of \sizeof{T} {\em array-select} operations.
A memory write \memWrite{\mem{}}{a}{v}{T} is expanded into \sizeof{T} nested {\em array-store} operations.
TODO encoding of points-to invariants i.e. $\pointsTo$

\subsection{Reconciliation of Counterexamples}
\label{sec:cerecons}

\input{chapters/figures/figReconcile.tex}

As detailed in \cref{sec:smtencoding}, each ADT variable $v$ gets lowered into a set of scalar
variables $\mathcal{F}(v)$ during SMT encoding.
Evidently, the models returned by SMT solvers map these variables (in $\mathcal{F}(v)$ instead of $v$)
to constant values.
We are interested in recovering a counterexample for the original query from a
model returned by the SMT solver.
Recall that, these counterexamples help guide the correlation search (in \cref{sec:searchalgo})
and invariant inference (in \cref{sec:invinferalgo}) procedures.
The process of constructing a constant for $v$ from the constant values returned for $\mathcal{F}(v)$
by an SMT solver is called {\em reconciliation}.
Obviously, the reconciled counterexample must be a valid counterexample to the original proof obligation.
$Reconcile(v:T, \gamma)$ is responsible for performing reconciliation for variable $v$ (of type $T$)
from the model $\gamma$ (returned by a SMT solver).
$Rand(T)$ returns an arbitrary constant of type $T$.
For example, consider the rather contrived proof obligation $P: {\tt true} \Rightarrow \sumIs{l}{\cons{LNil}}$.
Clearly, any valuation of $l$ where $l$ is a non-empty list is a valid counterexample to $P$.
However, a counterexample $\gamma$ returned by an SMT solver would instead contain
the mapping $\{ l \strsep{} tag \mapsto \mathcal{E}(\cons{LCons}) \}$.
During reconciliation, we find that $l \strsep{} tag$ is mapped to the data constructor \cons{LCons}
and recurse for each of its fields \cons{val} and \cons{tail}.
Since $\gamma$ do not contain a mapping for either of these fields, we soundly generate random constants
for these instead.
Indeed, $Reconcile$ correctly constructs a non-empty but otherwise arbitrary list for $l$, which
is a counterexample to $P$.

\subsection{Value Tree Representation}
\label{sec:valuegraph}
This section presents a graphical representation of expressions that helps us simplify the
implementation of multiple subprocedures used by our proof discharge algorithm.
This includes the process of canonicalization, reducing approximate \recursiveRelations{} as well as
construction of deconstruction programs as part of type III proof obligations.
We call this the {\em Value Tree} representation and use $\mathcal{V}(e)$ to denote a value tree associated with $e$.
We give an algorithm to convert an expression $e$ into $\mathcal{V}(e)$ and list its applications.

Before diving into value trees, we start by introducing an analogous (but simpler) representation for types, called {\em Type Trees}.
We use $\mathcal{T}(\tau)$ to denote a type tree associated with $\tau$.
% To define type trees, we begin with a formal description of ADTs.
Recall that ADTs are simply `sum of product' types where each data construction represents a variant (of the sum-type) and
each data construction contains values for each of its fields (of the product-type).
On top of ADTs, IR has build-in scalar types: \type{unit}, \type{bool} and \type{i<N>}.
Types in IR can be represented in {\em first order recursive types} \cite{recursivetypestrees} using the product ($\times$) and sum ($+$) type
constructors; and the scalar types (i.e. nullary type constructors).
The type system is characterized by the grammar \typegrammar{} as follows:

$$
T \rightarrow \mu \alpha. \ T \ |\ T \times \dots \times T \ |\  T + \dots + T \ |\  \type{unit} \ |\ \type{bool} \ |\  \type{i\langle N \rangle} \ |\ \alpha
$$

Every IR type can be encoded as a closed term (i.e. term without free variables) in \typegrammar{}.
For example, the \type{List} type can be written as $\mu \alpha. \type{unit} + (\type{i32} \times \alpha)$.
Note the use of a type variable $\alpha$ which is bound using $\mu$ to represent recursion.
Similarly, the \type{Matrix} type is represented by the term
$\mu \alpha. \type{unit} + ((\mu \beta. \type{unit} + (\type{i32} \times \beta)) \times \alpha)$,
where the type variables $\alpha$ and $\beta$ are used to bind recursive types \type{Matrix} and \type{List}
at their definitions respectively.

\input{chapters/figures/figTypeTrees.tex}

\Cref{fig:typetrees} shows the type trees for three ADTs \type{List}, \type{Tree}, and \type{Matrix} respectively.
In a type tree, each internal node represents either a product (\prodn{}) or a sum (\sumn{}) type constructor.
The leaf nodes are the scalar types.
Each outgoing edge of a \sumn{} node is associated with a data constructor of the corresponding ADT (i.e. \cons{LCons} for \type{List}).
Similarly, each outgoing edge of a \prodn{} node is associated with a field of the corresponding data constructor (i.e. \field{val} for \type{LCons}).
We assign integer indices to the internal nodes and use \ttedge{v}{label} to identify the edge outgoing at $v$ associated with {\tt label},
where {\tt label} is either a data constructor or a field name.
The root node is denoted by $v_0$.
The edges going outward from the root node are called {\em tree-edges} e.g., \ttedge{0}{LCons} and \ttedge{1}{val} in \cref{fig:typetreelist1}.
Edges that are not tree-edges, are called {\em back-edges} e.g., \ttedge{1}{cols} in \cref{fig:typetreematrix1}.
Every back-edge induces an unique simple cycle in the type tree representation.

\input{chapters/figures/figTypeTreesPeel.tex}

Recall that types in \SpecL{} (and in IR) follow equirecursive typing rules i.e. types
$\mu \alpha. T$ and $T[\mu \alpha. T/\alpha]$ in \typegrammar{} are {\em equal} types,
where $T[\mu \alpha. T/\alpha]$ represents the new type obtained by substituting all free
instances of $\alpha$ with $\mu \alpha. T$, and is defined as the {\em unfolding} of $\mu \alpha. T$.
In general, under equirecursive typing, two types are equal iff their infinite expansions (through unfolding) are equal.
In the type tree representation, two types are equal iff their infinite expansions are equivalent.
Such type trees are called isomorphic and two types are isomorphic iff they represent the same type.
An unfolding in the term representation corresponds to {\em unrolling} one iteration of a simple cycle in its type tree.
\Cref{fig:typetreespeel} shows three type trees for the \type{List} type.
\Cref{fig:typetreelistpeel1} corresponds to the canonical (intuitively the `smallest') type tree for the \type{List} type.
The type trees \cref{fig:typetreelistpeel2,fig:typetreelistpeel3} are obtained by {\em peeling} and unrolling
the back-edge \ttedge{1}{tail} (in \cref{fig:typetreelistpeel1}) respectively.
Peeling is a form of partial unrolling which only extracts the starting node of the cycle.
In practice, equality of two types (encoded in \typegrammar{}) can be reposed as syntactic
equality of their {\em canonical forms} \cite{canonicalrecursivetypes}.
In general, type trees may contain cycles (due to back-edges) and hence are not quite `trees'.
However, they represent the actual (possibly infinite) trees obtained through repeated unrolling of cycles.

\input{chapters/figures/figValueTrees.tex}

With type trees out of the way, we are ready to present their value analogue called `value trees'.
\Cref{fig:valuetrees} shows the value trees for three \type{List} expressions.
Note that, all three value trees are isomorphic to one of the
\type{List} type trees shown in \cref{fig:typetreespeel}, e.g.,
\cref{fig:valuetreelist2} is isomorphic to \cref{fig:typetreelistpeel2}.
In general, for an expression $e$ of type $\tau$, its value tree $\mathcal{V}(e)$ resembles
its type tree with the following distinctions:

\begin{enumerate}
\item Similar to a type tree, each internal node is either a \sumn{} or a \prodn{} node.
\item Instead of a scalar type $\tau_s$, each leaf node in $\mathcal{V}(e)$ contains an expression
of type $\tau_s$.
\item Similar to the Control-Flow Graph representation (presented in \cref{sec:cfg}), each node $v$
is associated with a symbolic state $\Omega_v$.
\item In addition to a data constructor, each edge originating at a \sumn{} node $v$ also contains
an edge condition expression (a boolean valued function over $\Omega_v$).
We identify such an edge with \vtedge{v}{\mathnormal{V}}{c}, where $v$ is the sum node,
$V$ is a data constructor and $c$ is the edge condition.
The set of edge conditions for all outgoing edges at a \sumn{} node must be mutually exclusive and exhaustive.
\item In addition to a field name, each edge $v \rightarrow v'$ originating at a \prodn{} node $v$ also contains
a transfer function ($\Omega_{v'}$ as a function of $\Omega_v$).
We identify such an edge with \vtedge{v}{fi}{\xfer{}}, where $v$ is the product node,
\field{fi} is a field name and \xfer{} is the transfer function.
\item Additionally, a value tree also contains a special node (called the {\em entry node}), and a special edge
(called the {\em entry edge}) from the entry node to $v_0$ (i.e. the root of the tree).
We use $\xi$ to denote the entry node.
The entry edge is associated with a transfer function $\xfer{}_\xi$.
We often omit the entry node in figures for brevity.
\item A value tree $\mathcal{V}(e)$ can be converted to a type tree $\mathcal{T}$ as follows:
(a) remove the entry node and edge pair, (b) remove edge conditions and transfer functions
associated with all edges, and (c) replace each leaf node expression of (scalar) type $\tau_s$
with $\tau_s$ itself.
The resulting type tree $\mathcal{T}$ represents the type $\tau$ of the expression $e$.
\end{enumerate}

Intuitively, a value tree simultaneously represents the value of the expression as well as
the CFG of its {\em abstracted} deconstruction program.
We will subsequently discuss these properties along with their applications in the context
of our proof discharge algorithm.
Next, we give an algorithm to convert an expression $e$ to its value tree representation $\mathcal{V}(e)$.

\subsection{Conversion of Expressions to their Value Trees}
\label{sec:valuetreeconv}
In this section, we present an algorithm to recursively construct a value tree for any arbitrary expression $e$.
We take a visual approach to match the graphical nature of value trees.

\begin{figure}[H]
\begin{subfigure}[b]{\textwidth}
\begin{center}
\includegraphics[scale=1.3]{chapters/figures/figValueTreeConvScalar.pdf}
\end{center}
\end{subfigure}
\caption{\label{fig:valuetreeconvscalar} Construction of $\mathcal{V}(e_1 \odot e_2)$ from $\mathcal{V}(e_1)$ and $\mathcal{V}(e_2)$.\\
$\odot$ represents an arbitrary scalar operator.}
\end{figure}

\subsubsection{Scalar Operators}
Given an expression $e = e_1 \odot e_2$,
\cref{fig:valuetreeconvscalar} shows the construction of $\mathcal{V}(e)$
from $\mathcal{V}(e_1)$ and $\mathcal{V}(e_2)$ respectively.
Since $e_1$ and $e_2$ have scalar types, their value trees must have exactly one
node (i.e. the leaf node) containing an expression ($e'_1$ and $e'_2$ respectively) of the same type.
$\odot$ represents an aribtrary scalar operator, i.e. an operator whose arguments
are scalar-typed values (e.g., bitvector arithmatic and relational operators).
Given an expression $s$ and a transfer function \xfer{}, $\xfer{}(s)$
represents the expression obtained by applying \xfer{}, interpreted as a substitution,
to $s$. This is equivalent to the weakest-precondition of $s$ along an edge associated
with the \xfer{}.
The construction shown in \cref{fig:valuetreeconvscalar} can be generalized to
$n$-ary scalar operators for $n>2$.

\begin{figure}[H]
\begin{subfigure}[b]{\textwidth}
\begin{center}
\includegraphics[scale=1.3]{chapters/figures/figValueTreeConvCons.pdf}
\end{center}
\end{subfigure}
\caption{\label{fig:valuetreeconvcons} Construction of $\mathcal{V}(\cons{LCons}(e_1,e_2))$ from $\mathcal{V}(e_1)$ and $\mathcal{V}(e_2)$.\\
$\mathcal{R}_\cons{LNil}$ represents an arbitrary value tree corresponding to the product-type (in \typegrammar{}) associated with \cons{LNil}.}
\end{figure}

\subsubsection{ADT Data Constructors}
Given an expression $e = \cons{LCons}(e_1, e_2)$,
\cref{fig:valuetreeconvcons} depicts the construction of $\mathcal{V}(e)$
from $\mathcal{V}(e_1)$ and $\mathcal{V}(e_2)$ respectively.
In general, for an arbitrary data constructor $V$ of ADT $T$,
the process begins with a \sumn{} node (0 in \cref{fig:valuetreeconvcons})
such that the outgoing edge associated with the value constructor $V$ (\cons{LCons} in \cref{fig:valuetreeconvcons})
has an edge condition of {\tt true} while all other edges are assigned the edge condition {\tt false}.
For each data constructor $V' \neq V$ of $T$, we append a random value tree corresponding to the product-type
associated with $V'$ in \typegrammar{}.
For example, given \type{List} is associated with the sum-type $\mu \alpha. \type{Unit} + (\type{i32} \times \alpha)$,
the product-types associated with \cons{LNil} and \cons{LCons} are:
$\type{Unit}$ and $\mu \alpha. \type{i32} \times (\type{Unit} + \alpha)$ respectively.
We use $\mathcal{R}_\tau$ to denote an arbitrary (i.e. random) value tree of type $\tau$.
For the outgoing edge associated with the data constructor $V$ (\vtedge{0}{LCons}{\tt true} in \cref{fig:valuetreeconvcons}),
we construct a product node (1 in \cref{fig:valuetreeconvcons}) and append the
value trees corresponding to the arguments $e_i$ as children of the product node.
% Interpreted as a program, all random value trees are appended under a {\tt false} edge condition
% (i.e. unreachable) and hence do not change the value represented by $\mathcal{V}(e)$.
% Their only responsibility is to keep the overall structure of $\mathcal{V}(e)$ isomorphic to $\mathcal{T}(\type{List})$.

\begin{figure}[H]
\begin{subfigure}[b]{\textwidth}
\begin{center}
\includegraphics[scale=1.3]{chapters/figures/figValueTreeConvSumIs.pdf}
\end{center}
\end{subfigure}
\caption{\label{fig:valuetreeconvsumis} Construction of $\mathcal{V}(\sumIs{e}{MCons})$ from $\mathcal{V}(e)$.\\
The dashed edge represents the (possibly empty) set of backedges originating in $v_2$ that terminates at 0.}
\end{figure}

\subsubsection{Sum-Is Operator}
Given a {\em sum-is} expression $e' = \sumIs{e}{MCons}$,
\cref{fig:valuetreeconvsumis} shows the construction of $\mathcal{V}(e')$
from $\mathcal{V}(e)$.
The process is rather straightforward and for a general expression $\sumIs{e}{\mathnormal{V}_i}$, entails extracting the
edge condition $c$ ($c_2$ in \cref{fig:valuetreeconvsumis}) from the $\mathcal{V}(\sumIs{e}{\mathnormal{V}_i})$ edge
\vtedge{v_0}{V}{c} (\vtedge{0}{MCons}{c_2} in \cref{fig:valuetreeconvsumis}).
Notice that the entry transfer function $\xfer{}_\xi$ during this conversion.

\begin{figure}[H]
\begin{subfigure}[b]{\textwidth}
\begin{center}
\includegraphics[scale=1.2]{chapters/figures/figValueTreeConvProdAccess.pdf}
\end{center}
\end{subfigure}
\caption{\label{fig:valuetreeconvprodaccess} Construction of $\mathcal{V}(\prodAccess{e}{cols})$ from $\mathcal{V}(e)$.\\
Similar to type trees, {\tt unroll} \ttedge{1}{cols} represents the operation of hoisting one iteration of the cycle \pathset{0,1,0}.}
\end{figure}

\subsubsection{Product-Access Operator}
Given an expression $e' = \prodAccess{e}{cols}$,
\cref{fig:valuetreeconvprodaccess} depicts the construction of $\mathcal{V}(e')$
from $\mathcal{V}(e)$.
Intuitively, $\mathcal{V}(e')$ represents the subtree of $\mathcal{V}(e)$ rooted at the \sumn{} node reached by taking
the edges \vtedge{0}{MCons}{c_2} followed by \vtedge{1}{cols}{\Omega_2}.
However, this path may contain backedges or the subtree itself may contain backedges leaving the subtree.
In such a case, we perform peeling until all such backedges strickly terminate within this subtree.
For example, in \cref{fig:valuetreeconvprodaccess}, the edge \vtedge{1}{cols}{\Omega_2} is a backedge and hence we peel it once.
In the resulting (equivalent) value tree, the subtree (rooted at 2) contains a backedge leaving the subtree which requires
one more peeling operation.
The resulting value tree contains the subtree rooted at the \sumn{} node 2 which satisfies the two conditions above and
hence $\mathcal{V}(e')$ is simply constructed by extracting the subtree rooted at \sumn{} node 2.
Note that we preserve the transfer functions from the entry to the \sumn{} node 2 during extraction.
$\xfer{}_1 \cdot \xfer{}_2$ represents the composition of the transfer functions $\xfer{}_1$ and $\xfer{}_2$ respectively.

\begin{figure}[H]
\begin{subfigure}[b]{\textwidth}
\begin{center}
\includegraphics[scale=1.3]{chapters/figures/figValueTreeConvIte.pdf}
\end{center}
\end{subfigure}
\caption{\label{fig:valuetreeconvite} Construction of $\mathcal{V}($ \sumIf{c} \sumThen{\cons{MNil}} \sumElse{\cons{MCons}(e_1,e_2)} $)$ from $\mathcal{V}(c)$, $\mathcal{V}(e_1)$ and $\mathcal{V}(e_2)$.}
\end{figure}

\subsubsection{\underline{If}-\underline{Then}-\underline{Else} Operator}
Given an expression $e =$ \sumIf{c} \sumThen{\cons{MNil}} \sumElse{\cons{MCons}(e_1,e_2)},
\cref{fig:valuetreeconvite} describes the construction of $\mathcal{V}(e)$
using $\mathcal{V}(c)$, $\mathcal{V}(e_1)$ and $\mathcal{V}(e_2)$.
Let us consider a general \sumDtor{} expression $e$ (associated with the ADT $T$ with data constructors $V_1,V_2,\dots,V_n$)
such that the branch associated with $V_i$ is given by $V_i(e_i^1,e_i^2,\dots)$.
We begin with the construction of a \sumn{} root node (0 in \cref{fig:valuetreeconvite})
such that the outgoing edge associated with $V_i$ has the edge condition equal to the
expression path condition of the branch $V_i(e_i^1,e_i^2,\dots)$
($\xfer{}(c')$ and $\neg \xfer{}(c')$ for \cons{MNil} and \cons{MCons} respectively in \cref{fig:valuetreeconvite}).
For each outgoing edge associated with the data constructor $V_i$, we construct a \prodn{} node
(1 for \cons{MCons} in \cref{fig:valuetreeconvite}) and append the value trees corresponding to
the arguments $e_i^j$ as its children.

\begin{figure}[H]
\begin{subfigure}[b]{\textwidth}
\begin{center}
\includegraphics[scale=1.3]{chapters/figures/figValueTreeConvLift.pdf}
\end{center}
\end{subfigure}
\caption{\label{fig:valuetreeconvlift} Construction of $\mathcal{V}(\lifted{list}{\mem{}}{lnode}{p})$ from $\mathcal{V}(p)$.\\
The process involves assuming $v_\vtse{1}^\lift{list}{}{}$ to be the value tree of $\lifted{list}{\mem{}}{lnode}{\vtse{1}}$,
followed by expansion of $v_\vtse{1}^\lift{list}{}{}$ using the definition of \lift{list}{\mem{}}{lnode} (in \cref{eqn:clist})
and, finally folding the tree-edge \ttedge{1}{tail} incident on the self-referencial subtree $v_\vtse{1}^\lift{list}{}{}$ into a backedge.}
\end{figure}

\subsubsection{Lifting Constructor}
Given an expression $e = \lifted{list}{\mem{}}{lnode}{p}$,
\cref{fig:valuetreeconvlift} shows the construction of $\mathcal{V}(e)$
from $\mathcal{V}(p)$.
Recall the recursive definition of the lifting constructor \lift{list}{\mem{}}{lnode} given in
\cref{eqn:clist}.
We start by assuming that $v_\vtse{1}^\lift{list}{}{}$ is the value tree for the lifted expression
\lifted{list}{\mem{}}{lnode}{\vtse{1}}.
Hence, the value tree of \lifted{list}{\mem{}}{lnode}{p} is identical to $v_\vtse{1}^\lift{list}{}{}$
except we assign the actual argument (i.e. $\xfer{}(p')$) to the formal argument $\vtse{1}$ along the
entry edge.
Next, we expand the subtree $v_\vtse{1}^\lift{list}{}{}$ based on the unrolling procedure of \lift{list}{\mem{}}{lnode}
(defined in \cref{eqn:clist}) until the entire value tree becomes a self-referencial structure.
For example, after expanding through \cref{eqn:clist} once in \cref{fig:valuetreeconvlift},
the value tree contains an tree-edge \ttedge{1}{tail} incident on the self-referencial subtree $v_\vtse{1}^\lift{list}{}{}$.
In the last step, we fold all self-referencial tree-edges (\ttedge{1}{tail} in \cref{fig:valuetreeconvlift})
by converting them into backedges terminating at the root of the subtree being referenced
(0 for $v_\vtse{1}^\lift{list}{}{}$ in \cref{fig:valuetreeconvlift}).

\begin{figure}[H]
\begin{subfigure}[b]{\textwidth}
\begin{center}
\includegraphics[scale=1.2]{chapters/figures/figValueTreeConvVar.pdf}
\end{center}
\end{subfigure}
\caption{\label{fig:valuetreeconvvar} Construction of $\mathcal{V}(m)$ for a \type{Matrix} variable $m$.
The process is identical to the construction of value trees for lifted expressions as shown in \cref{fig:valuetreeconvlift}.}
\end{figure}

\subsubsection{Variables}
Finally, we are interested in constructing the value tree for a variable.
Recall that, every ADT (pseudo-)variable is associated with an unrolling procedure
characterized by the ADT itself. e.g. \cref{eqn:specDeconstruct}
for the \type{List} variable $l$.
The \type{Matrix} ADT is defined as:
$\type{Matrix} = \cons{MNil} \ | \ \cons{MCons}(\type{List},\type{Matrix})$,
and thus the unrolling procedure for a \type{Matrix} variable $m$ is given by:

\begin{equation}
\label{matrixunrollingprocedure}
m = \sumIf{\sumIs{m}{MNil}} \  \sumThen{\cons{MNil}} \  \sumElse{\cons{MCons}(\prodAccess{m}{row}, \prodAccess{m}{cols})}
\end{equation}

\Cref{fig:valuetreeconvvar} illustrates the construction $\mathcal{V}(m)$ for the \type{Matrix} variable $m$.
The process consists of the same three steps used to construct the value tree
for a lifted expression -- assume, expand and fold.
First, we assume that $v_\vtse{1}^\type{Mat}$ and $v_\vtse{2}^\type{Lis}$ are the value trees
corresponding to the pseudo-variables \vtse{1} and \vtse{2} of \type{Matrix} and \type{List}
types respectively.
Thus, $\mathcal{V}(m)$ is equal to $v_\vtse{1}^\type{Mat}$ with the entry edge transfer function
$\{ \vtse{1} \mapsfrom m \}$.
We expand the definitions of $v_\vtse{1}^\type{Mat}$ and $v_\vtse{2}^\type{Lis}$ once each before
the value tree becomes self-referencial.
Finally, we fold the treeedges \ttedge{1}{cols} and \ttedge{3}{tail} into the back-edges
terminating at the roots of the subtrees representing $v_\vtse{1}^\type{Mat}$ and $v_\vtse{2}^\type{Lis}$
respectively (nodes 0 and 3 in \cref{fig:valuetreeconvvar}).

\subsection{Applications of Value Trees}
\label{sec:valuetreeapps}
With the construction algorithm out of the way, we next discuss some of the applications of value tree representation
in the context of our proof discharge algorithm.

\subsubsection{Canonical Form Property}
For any expression $e$, its value tree $\mathcal{V}(e)$ has the following property:
for any path from the entry node $\xi$ to a leaf node $v_l$ (containing a scalar expression $s_l$)
$\rho[\xi \rightarrow \dots \rightarrow v_l]$, the weakest-precondition of $s_l$ at the entry $\xi$
is always in canonical form.
This is called the {\em canonical form property} of value trees.
The proof of this property follows from the construction algorithm described in \cref{sec:valuetreeconv}.
Let $P$ be a proof obligation of \type{bool} type.
Recall that, a value tree for a scalar type $T$ (such as \type{bool}) must be composed of a single leaf node at its root
(say $v_0$) containing a scalar expression (say $s$) of type $T$ along with a transfer function $\Omega$ associated
with the entry edge.
The expression $\Omega(s)$ is equal to the proof obligation $P$.
Due to the canonical form property, $\Omega(s)$ is also in the canonical form.
Hence, we can convert a proof obligation $P$ (without \recursiveRelations{}) to its canonical form
by converting to value tree followed by extracting the expression at the root.

\subsubsection{Reduction of Approximate Recursive Relations}
Recall that, in \cref{sec:valuetreeconv} we did not give an algorithm for constructing the value
tree for a \recursiveRelation{} $l_1 \indEq{} l_2$.
Unlike $l_1 \indEq{} l_2$, the value trees corresponding to its $d$-depth
over- and under-approximations ($l_1 \indEqDepth{d} l_2$ and $l_1 \indEqUapprox{d} l_2$ respectively)
are construcible from $\mathcal{V}(l_1)$ and $\mathcal{V}(l_2)$.

Since an ADT represents a `sum of product' type, each level of an ADT value (in its expression tree)
corresponds to two levels -- \sumn{} and \prodn{} (in the value tree).
Hence, a $d$-depth over-approximation of $l_1 \indEq{} l_2$ simply asserts equality of all
leaf expressions between $\mathcal{V}(l_1)$ and $\mathcal{V}(l_2)$, up to a depth of $2d$.
Similarly, the $d$-depth under-approximation of $l_1 \indEq{} l_2$ asserts the above,
and in addition asserts the unreachability of all paths outgoing at level $2d$ i.e. paths
incident at a depth of $2d+1$.
Due to the canonical form property, this allows us to convert any proof obligation $P$ without
a \recursiveRelation{} (but possibly its approximations) to its canonical form
by converting it to the value tree representation, followed by extracting the expression at the root.

\subsubsection{Bisimilarity of Value Trees}
Recall that, $l_1 \indEq{} l_2$ asserts exact equality of $l_1$ and $l_2$ up to an arbitrary depth.
Using the program representation of value graphs, if we are able to establish bisimilarity
between $\mathcal{V}(l_1)$ and $\mathcal{V}(l_2)$
(similar to bisimilarity between their deconstruction programs),
we have shown that $l_1 \indEq{} l_2$ holds.
We can interpret value trees as non-deterministic Control Flow Graphs.
The inputs include the free variables at the entry node $\xi$ and
the observable outputs are the expressions contained in the leaf nodes.
To make the search for a bisimulation easier, we can peel the value trees of $l_1$ and $l_2$
to unify their tree structures.
Next, the bisimulation relation requires us to prove that the outputs of
each pair of leaf nodes are identical.
Just like their deconstruction programs, we run our points-to analysis before the check
for bisimilarity, to identify points-to invariants to help in the bisimulation process.
Recall the \rhs{} of the type III proof obligation illustrated in \cref{sec:cat3}:
$\lhs{} \Rightarrow \lifted{list}{\mem{}}{lnode}{\cv{l}} \indEq{} \lifted{list}{\mem{}'}{lnode}{\cv{l}}$.
Also recall that the points-to invariants available at \cpc{5} are (showing only the relevant ones):
$\cv{p} \pointsTo{} \{ \mlrf{\cpc{4}} \}$
$\cv{l} \pointsTo{} \{ \mlrs{\cpc{4}} \}$, $\mlrf{\cpc{4}} \pointsTo{} \{ \mlrs{\cpc{4}} \}$,
$\mlrs{\cpc{4}} \pointsTo{} \{ \mlrs{\cpc{4}}, \heapr{} \}$, and
$\heapr{} \pointsTo{} \{ \mlrs{\cpc{4}}, \heapr{} \}$.
\Cref{fig:valuetreebisim} shows the value trees of $\lifted{list}{\mem{}}{lnode}{\cv{l}}$
and $\lifted{list}{\mem{}'}{lnode}{\cv{l}}$ along with the table of invariants
required by the proof of equivalence through bisimulation.

\input{chapters/figures/figValueTreeBisim.tex}
