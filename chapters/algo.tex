\section{Spec-to-C Equivalence Checker}
\label{sec:spectocalgo}
In this section, we present our automatic equivalence checker algorithm \toolName{}.
\toolName{} is able to search for a bisimulation based proof of equivalence
between \SpecL{} and C programs.
We start with a dataflow formulation of our points-to analysis.
Recall that the points-to analysis is used to identify may-point-to invariants in the C program,
as well as deconstruction programs.
As described in \cref{sec:contribs}, \toolName{} is based on three primary algorithms:
(a) an algorithm to incrementally construct a product-CFG by correlating program executions across
the \SpecL{} and C procedures respectively,
(b) an algorithm to identify inductive invariants at intermediate PCs in the (partially constructed)
product-CFG, and (c) an algorithm for solving proof obligations generated by the first two algorithms.
The last section illustrates our proof discharge algorithm through sample proof obligations.
We describe our counterexample-guided best-first search algorithm for
construction of a product-CFG in \cref{sec:searchalgo}.
This is followed by a dataflow formulation of our counterexample-guided invariant inference algorithm in \cref{sec:invinferalgo}.
We finish with a comprehensive analysis of our proof discharge algorithm and its related subprocedures.

\input{chapters/figures/figPointstoDFA.tex}

\subsection{Points-to Analysis}
\label{sec:pointsToFormal}
Recall that in \cref{sec:reconsbisim}, we needed to reason about aliasing to successfully discharge a type III proof obligation.
These aliasing relationships are described in \cref{sec:pointsTo} and used in \cref{sec:pointsToAsInvariants} to
successfully discharge the proof obligation.
A points-to analysis is used to identify these aliasing relationships in \cprog{} as well as each deconstruction program \dprog{}.
We present a dataflow formulation of our points-to analysis as shown in \cref{tab:pointstoalgodfa}.
We start by identifying the set \memregions{} of all region labels representing mutually non-overlapping
regions of the $C$ memory state \mem{}.
For each call to {\tt malloc()} at PC $A$, we add $A_1$ and $A_{2+}$ to \memregions{}.
Recall that $A_1$ represents the region of memory returned by the {\em most recent} execution of $A$.
$A_{2+}$ represents the region of memory returned by older (i.e. all but most recent) executions of $A$.
$\memregions{} = \bigcup_{A} \{ A_1, A_{2+} \} \cup \{ \heapr{} \}$,
where \heapr{} is the region of memory \mem{} not covered by the labels associated with allocation sites.\footnote{TODO: per procedure or global?}

Let $\pseudoregs{}^{\cprog{}}$  be the set of all scalar pseudo-registers in \cprog{}.
We use a forward dataflow analysis to identify a may-point-to function
$\Delta^{\cprog{}}: (\pseudoregs{}^{\cprog{}} \cup \memregions{}) \mapsto 2^{\memregions{}}$ at each program point in \cprog{}.
For a deconstruction program \dprog{}, we are also interested in finding the may-point-to function for
all scalar pseudo-registers in \dprog{}, say $\pseudoregs{}^{\dprog{}}$.
Thus, \dprog{} contains mappings for $\Delta^{\dprog{}}$ as well:
$\Delta^{\dprog{}}: (\pseudoregs{}^{\cprog{}} \cup \memregions{} \pseudoregs{}^{\dprog{}}) \mapsto 2^{\memregions{}}$.
The '$\pointsTo$' operator introduced in \cref{sec:pointsTo} is called the {\em element-wise} may-point-to function
and is related to the may-point-to function $\Delta$ as follows: $p \pointsTo{} S \Leftrightarrow \Delta(p) = S$.

The meet operator is element-wise set-union e.g., $p \pointsTo{} S_1$ and $p \pointsTo{} S_2$
combines into $p \pointsTo{} S_1 \cup S_2$.
Evidently, the $\top$ value is the constant function that returns $\emptyset$.
At entry of \cprog{}, we conservatively assume that all memory regions may point to each other.
However, at entry of a deconstruction program \dprog{}, created during a proof obligation at product-CFG node $(n_S\!:\!n_C)$,
we use the precomputed \cprog{}'s may-point-to function at $n_C$ ($\Delta^{\cprog{}}_{n_C}$)
to initialize the points-to relationships for all state elements of \cprog{} (i.e. $(\pseudoregs{}^{\cprog{}} \cup \memregions{})$).
This is a crucial step for proving equality of \cprog{} values under different memory states as seen in \cref{sec:pointsToAsInvariants}.

Next, we discuss the transfer function $f_e$ for our points-to analysis.
For an IR instruction ${\tt x} \coloneqq {\tt c}$, for constant {\tt c}, the
transfer function updates $\Delta({\tt x}) \coloneqq \emptyset$.
For instruction ${\tt x} \coloneqq {\tt y\ op\ z}$ (for some arithmetic or logical operator {\tt op}),
we update $\Delta({\tt x}) \coloneqq \Delta({\tt y}) \cup \Delta({\tt z})$.
For a load instruction ${\tt x} \coloneqq \memRead{\mem{}}{y}{T}$, we
update $\Delta({\tt x}) \coloneqq \bigcup_{t \in \Delta(y)} \Delta(t)$.
For a store instruction $\mem{} \coloneqq \memWrite{\mem{}}{x}{y}{T}$, for all
$t \in \Delta({\tt x})$, we update $\Delta(t) \coloneqq \Delta(t) \cup \Delta(y)$.
For a malloc instruction ${\tt x} \coloneqq {\tt malloc}_A()$
(where $A$ represents the allocation site), we perform the following steps (in order):
\begin{enumerate}
\item Convert all existing occurrences of $A_1$ to $A_{2+}$, i.e., for all $t \in (\pseudoregs{}^{\cprog{}} \cup \memregions{})$,
if $A_1 \in \Delta(t)$, then update $\Delta(t) \coloneqq (\Delta(t) \setminus \{ A_1 \}) \cup \{ A_{2+} \}$.
\item Update $\Delta({\tt x}) \coloneqq \{ A_1 \}$.
\item Update $\Delta(A_{2+}) \coloneqq \Delta(A_{2+}) \cup \Delta(A_1)$.
\item Update $\Delta(A_1) \coloneqq \emptyset$.
\end{enumerate}

For function calls, a {\em supergraph} is created by adding control flow edges
from the call-site to the procedure head (copying actual arguments to the formal arguments) and
from the procedure exit to the program point just after the
call-site (copying returned value to the variable assigned at the callsite),
e.g., in \cref{fig:recons}, the dashed edges represent supergraph edges.

The allocation-site abstraction (with a bounded-depth call stack) is
known to be effective at disambiguating memory regions belonging to
different data structures
\cite{allocationSiteAbstraction82,allocationSiteAbstraction90,allocationSiteAbstraction06}.
In our work, we also need to reason about non-aliasing
of the most-recently allocated object (through a {\tt malloc} call) and
the previously-allocated objects (as in the \type{List}
construction example). The coarse-grained $\{1, 2+\}$
categorization of allocation recency is effective for such disambiguation.

\subsection{Counterexample-guided Product-CFG Construction}
\label{sec:searchalgo}
\toolName{} constructs a product-CFG incrementally to search for an observably-equivalent
bisimulation relation between the CFGs of a \SpecL{} procedure \sprog{} and a C procedire \cprog{}.
Multiple candidate product-CFGs are partially constructed during this search;
the search completes when one of these candidates yield an equivalence proof.

{\em Anchor nodes} are identified in the CFGs of \sprog{} and \cprog{}, and represents the
source and destination nodes (i.e. IR PCs)
of paths chosen to be correlated between the two programs.
The algorithm ensures that every cycle in both \sprog{} and \cprog{} contains at least one anchor node.
The start and exit nodes are always anchor nodes.
Also, for every function call, the nodes just before and after its callsite are considered anchor nodes.
For example, in \cref{fig:llAllocCCFG}, \cpc{4} and \cpc{5} are anchor nodes around the call to {\tt malloc}.
The selected anchor nodes for the CFGs in \cref{fig:llAllocSpecIRCFG,fig:llAllocCCFG} are:
$\{ \spc{0},\spc{3},\spc{E} \}$ and $\{ \cpc{0}, \cpc{3}, \cpc{4}, \cpc{5}, \cpc{E} \}$ respectively.
For each anchor node in \cprog{}, our search algorithm searches for a correlated anchor node in \sprog{} --- if
a (partially constructed) product-CFG $\pi$ contains a product-CFG node  $(n_S\!:\!n_C)$, then $\pi$
correlates node $n_C$ in \cprog{} with node $n_S$ in \sprog{}.
The search procedure begins with a single partially-constructed product-CFG $\pi_{init}$.
$\pi_{init}$ contains exactly one node (\scpc{0}{0}) that encodes the correlation of the entry nodes
(i.e. \spc{0} and \cpc{0}) of \sprog{} and \cprog{}.

At each step of the incremental construction process, a node $(n_S\!:\!n_C)$ is chosen in a product-CFG $\pi$
and a path $\rho_C$ in \cprog{} starting at $n_C$ (and ending at an anchor node in \cprog{}) is selected.
Then, we enumerate potentially correlated paths in \sprog{} for the path $\rho_C$ in \cprog{}.
For example, during construction of the product-CFG shown in \cref{fig:llAllocProductCFG},
say we select the product-CFG node (\scpc{3}{3}).
We choose the \cprog{} path \cpath{3,4} and enumerate its potential correlations (i.e. paths in \sprog{} starting at \spc{3}):
$\epsilon$, \spath{3,5,3}, \spath{3,5,3,5,3}, \ldots, $\pathset{S3,(\pathset{S5,S3})^\mu}$.
The {\em unroll factor} $\mu$ is a fixed parameter of the algorithm and represents the maximum number of iterations of a loop (in \sprog{}),
that may be correlated with a path $\rho_C$ in \cprog{}.
Importantly, for paths $\rho_S$ (in \sprog{}) and $\rho_C$ (in \cprog{}) to be considered for correlation,
they must begin and end at anchor nodes, i.e. the path \spath{3,5} is skipped during enumeration.
Moreover, the path $\rho_C$ may not contain anchor nodes in the middle.
Hence, the path \cpath{3,4,5} is not considered for $\rho_C$,
instead we attempt to correlate the subpaths \cpath{3,4} and \cpath{4,5} individually.

For each enumerated correlation possibility $(\rho_S,\rho_C)$, a separate product-CFG $\pi'$ is
created (by cloning $\pi$) and a new product-CFG edge $e=(\rho_S,\rho_C)$ is added to $\pi'$.
The head of the product-CFG edge $e$ is the (potentially newly added) product-CFG node representing
the correlation of the end-points of paths $\rho_S$ and $\rho_C$. For example, the node (\scpc{3}{4}) is added
to the product-CFG if it correlates paths $\epsilon$ and \cpath{3,4} starting at (\scpc{3}{3}).
For each node $s$ in a product-CFG $\pi$, we maintain a small number of
concrete machine state pairs (of \sprog{} and \cprog{}).
The concrete machine state pairs at $s$ are obtained as counterexamples to an unsucessful proof
obligation \hoareTriple{\phi_s}{s \rightarrow d}{\phi_d} (for some edge $s \rightarrow d$ and node $d$ in $\pi$).
Thus, by construction, these counterexamples represent concrete state pairs that may potentially occur
at $s$ during the lockstep execution encoded by $\pi$.

To evaluate the promise of a possible correlation $(\rho_S,\rho_C)$ starting at node $s$
in product-CFG $\pi$, we examine the execution behaviour of the counterexamples at $s$ on
the product-CFG edge $e=(s\rightarrow d)=(\rho_S,\rho_C)$.
If the counterexamples ensure that the machine states remain related at $d$,
then that candidate correlation is ranked higher.
This ranking criterion is based on prior work \cite{oopsla20}.
A best-first search (BFS) procedure based on this ranking criterion is used to incrementally construct
a product-CFG (starting from $\pi_{init}$).
For each intermediate candidate product-CFG $\pi$ generated during this search procedure,
an automatic invariant inference procedure (discussed next in \cref{sec:invinferalgo}) is
used to identify invariants at all the nodes in $\pi$.
The counterexamples obtained from the proof obligations generated by this invariant inference
procedure are added to the respective nodes in $\pi$; these counterexamples help rank
future correlations starting at those nodes.

If after invariant inference, we realize that an intermediate candidate product-CFG $\pi_1$
is not promising enough, we backtrack and choose another candidate product-CFG $\pi_2$
and explore the potential correlations that can be added to $\pi_2$.
Thus, a product-CFG is constructed one edge at a time.
If at any stage, a product-CFG $\pi$ contains correlations for every path in \cprog{}
and invariants ensure equal observables (i.e. \post{} holds at correlated exit nodes),
we have successfully shown equivalence.
This counterexample-guided BFS procedure is similar to the one described in prior work on
the Counter algorithm \cite{oopsla20}.

\subsubsection{Correlation in the Presence of Procedure Calls}
\label{sec:correlfcalls}
Recall that a procedure $\delta$ in \sprog{} and \cprog{} may make function calls (including self calls),
e.g., allocation of memory in C, traversal of a tree data structure.
Recall that the nodes just before and after a function call are always considered anchor nodes.
Calls to memory allocation functions in \cprog{} (i.e. {\tt malloc}) are handled by correlating
the function call edge with the empty path ($\epsilon$) in \sprog{}.
For example, in the product-CFG shown in \cref{fig:llAllocProductCFG}, the {\tt malloc} edge \cpath{4,5} in \cprog{}
is correlated with $\epsilon$ in \sprog{}.

For all other calls, our correlation algorithm (in \cref{sec:searchalgo}) ensures that the anchor nodes
around such a callsite are correlated one-to-one across both procedures.
For example, let there be a call to procedure $\delta'$ in \sprog{} at PC $n_S$, i.e. $n_S$ is the call-site.
Let us denote the program point just after this call-site as $n'_S$.
Let {\tt args}$_{n_S}$ represent the values of the actual arguments of this function call (at $n_S$).
Let {\tt ret}$_{n'_S}$ represent the value returned by this function call (at $n'_S$).
Similarly, for a procedure call $\delta'$ in \sprog{}, let $n_C$, $n'_C$, {\tt args}$_{n_C}$ and {\tt ret}$_{n'_C}$
represent the function call call-site, program point just after the call-site,
the values of the actual arguments and the value returned respectively.
Our algorithm ensures that the only correlation possible in a product-CFG $\pi$ for these program points are
$(n_S:n_C)$ and $(n'_S:n'_C)$.

We utilize the user-supplied input-output specification for $\delta'$ (say $(\pre{}_{\delta'},\post{}_{\delta'})$)
to obtain the desired invariants at nodes $(n_S:n_C)$ and $(n'_S:n'_C)$ in the product-CFG.
A successful proof must {\em ensure} that $Pre_{\delta'}$({\tt args}$_{n_S}$,{\tt args}$_{n_C}$,$\mem{}_{n_C}$)
holds at $(n_S:n_C)$.
Further, the proof can {\em assume} that $Post_{\delta'}$({\tt ret}$_{n'_S}$,{\tt ret}$_{n'_C}$,$\mem{}_{n'_C}$)
holds at $(n'_S:n'_C)$.
Here, $\mem{}_{n_C}$ and $\mem{}_{n'_C}$ represents the memory states in \cprog{} at $n_C$ and $n'_C$ respectively.
Thus, for function calls, we inductively prove the precondition (on the arguments) at $(n_S:n_C)$
and assume the postcondition (on the returned values) at $(n'_S:n'_C)$.

\input{chapters/figures/figSearch.tex}

\input{chapters/figures/figInvariantDFA.tex}
\input{chapters/figures/figTfAndInvGrammar.tex}

\subsection{Invariant Inference and Counterexample Generation}
\label{sec:invinferalgo}
We formulate our counterexample-guided invariant inference algorithm as a dataflow analysis
as shown in \cref{tab:invinferalgodfa}.
The invariant inference procedure is responsible for inferring invariants $\phi_n$ at each intermediate
node $n$ of a (partially constructed) product-CFG, while also generating a set of counterexamples
$\Gamma_n$ that represents the potential concrete machine states at $n$.

Given the invariants and counterexamples at node $s$: ($\phi_s,\Gamma_s$),
the transfer function initializes the new candidate set of counterexamples at $d$ ($\Gamma^{can}_{d}$)
with the current set of counterexamples at $d$ ($\Gamma_{d}$) {\em union}-ed with
the counterexamples obtained by executing $\Gamma_s$ on edge $e$ (through {\tt exec}$_e$).
The candidate invariant at $d$ ($\phi^{can}_d$) is computed as the strongest cover
of $\Gamma^{can}_{d}$ ($StrongestInvCover()$).
At each step, the transfer function attempts to prove $\{\phi_s\} (e) \{\phi^{can}_d\}$
(through a call to $Prove()$).
If the proof succeeds ($Prove()$ returns \cons{True}), the candidate invariant $\phi^{can}_d$ is returned along with
the counterexamples $\Gamma^{can}_d$ learned so far.
Otherwise, $Prove()$ returns $\cons{False}(\gamma_s)$.
The candidate invariant $\phi^{can}_d$ is weakened using the counterexamples obtained
(i.e. $\gamma_s$) and the proof attempt is repeated.

The candidate invariants are drawn from the predicate grammar \invgrammar{} shown in \cref{fig:invinfergrammar}.
In addition to affine and inequality relations between bitvectors in \sprog{} and \cprog{},
\invgrammar{} supports \recursiveRelations{} between an ADT variable in \sprog{} and a lifted expression in \cprog{}.
The candidate lifting constructors (i.e. \lift{lift}{}{T}) are derived from the lifting constructors
present in the precondition \pre{} and the postcondition \post{}, as supplied by the user.
More sophisticated strategies for inference of new lifting constructors is possible.

$StrongestInvCover()$ for affine relations involve
identifying the basis vectors of the kernel of the
matrix formed by the counterexamples in the bitvector
domain \cite{esop05,semalign}.
For inequality relations, $StrongestInvCover(\Gamma)$
returns {\em true} (i.e. the weakest invariant) iff any counterexample in $\Gamma$ evaluates the
relation to false --- this effectively simulates the Houdini approach \cite{houdini}.
Similarly, in case of a \recursiveRelation{} $l_1 \indEq{} l_2$, $StrongestInvCover(\Gamma)$
returns {\em true} iff any counterexample in $\Gamma$ evalutes its $\eta$-depth over-approximation
$l_1 \indEqDepth{\eta} l_2$ to false, where $\eta$ is a fixed parameter of the algorithm.

\subsection{Proof Discharge Algorithm}
\label{sec:proofalgo}

\subsubsection{Summary of Canonicalization Procedure}
\label{sec:canonicalalgo}
\Cref{algo:canonical} shows the pseudo-code for the canonicalization procedure.
$Canonicalize(e)$ is responsible for converting an expression $e$ to its canonical form $\hat{e}$ (introduced in \cref{sec:unifyandrewrite}).
Recall that a pseudo-variable is an expression of the form $\prodAccess{v}{a_1,a_2,.,a_n}$, where $v$ is a variable.
Also recall that, an expression $e$ is canonical iff each {\em accessor} and {\em sum-is} expression operate on a pseudo-variable.
An ADT expression with a data constructor, a lifting constructor or the \sumDtor{}-operator at its top-level, is called a {\em foldable} expression.
$Canonicalize(e)$ iteratively folds each {\em accessor} and {\em sum-is} subexpressions of $e$ that operate on a foldable argument.
Thus, $Canonicalize(e)$ returns an expression where none of the {\em accessor} or {\em sum-is} subexpressions is foldable.
This condition implies the requirements of the canonical form.
For example, $a + \prodAccess{\cons{LCons}(b,l)}{tail,val}$ and $\sumIs{\lifted{list}{\mem{}}{lnode}{p}}{LNil}$
canonicalizes to $a + \prodAccess{l}{val}$ and $(p = 0)$ respectively.

\input{chapters/figures/figCanonical.tex}

\subsubsection{Summary of Unification Procedure}
\label{sec:unifalgo}
\Cref{algo:unification} shows the pseudo-code for the unification algorithm introduced in \cref{sec:unifyandrewrite}.
$\theta(p_1,e_1,p_2,e_2)$ is responsible for unifying expressions $e_1$ and $e_2$ under the expression path
conditions $p_1$ and $p_2$ respectively.
$\theta$ either fails to unify with the \cons{Fail} output, or it successfully returns $\cons{Succ}(S)$, where $S$
is the set of correlation tuples that relate (a) either two atomic expressions, or (b) an atom with an non-atomic expression.
$\theta(p_1,e_1,p_2,e_2)$ terminates when one of $e_1$ and $e_2$ is an atomic expression.
In case both $e_1$ and $e_2$ contains a data constructor at their top-level, 
$\theta$ attempts to recursively unify the data constructors and their corresponding children.
If exactly one of $e_1$ and $e_2$ is a \sumDtor{} expression,
$\theta$ attempts to unify both branches of \sumDtor{} (along with the path conditions) with the other expression
and return whichever succeeds.
If both $e_1$ and $e_2$ are \sumDtor{} expressions, $\theta$ attempts to recursively unify their children.
$\theta$ uses the $\sqcup$-operator to combine the results of successive self-calls.
$A \sqcup B$ is equal to $\cons{Succ}(S_1 \cup S_2)$ if $A = \cons{Succ}(S_1)$ and $B = \cons{Succ}(S_2)$;
otherwise (if one of $A$ and $B$ is \cons{Fail}), $A \sqcup B = \cons{Fail}$.

\input{chapters/figures/figUnification.tex}

\subsubsection{Summary of Iterative Unification and Rewriting Procedure}
\label{sec:unifyandrewritealgo}
\Cref{algo:unifyandrewrite} shows the pseudo-code for the iterative unification and rewriting procedure
introduced in \cref{sec:unifyandrewrite}.
$\Theta(p_a,e_a,p_b,e_b)$ is responsible for unifying expressions $e_a$ and $e_b$ under the expression
path conditions $p_a$ and $p_b$ respectively.
$\Theta$ either fails to unify with the \cons{Fail} output, or it successfully returns $\cons{Succ}(S)$, where $S$
is the set of correlation tuples that relate {\em only} atomic expressions.
$\Theta$ attempts to iteratively (a) unify the expressions (through a call to the unification procedure $\theta$ in \cref{sec:proofalgo}),
and (b) perform rewriting (of atom $a_1$ for those correlation tuples \corrtuple{p_1}{a_1}{p_2}{e_2} where $e_2$ is non-atomic), followed by
a recursive call to $\Theta$.
A \recursiveRelation{} $l_1 \indEq{} l_2$ is decomposed through the top-level invocation of $\Theta(true,l_1,true,l_2)$.

\input{chapters/figures/figIterUnifyAndRewrite.tex}

\subsubsection{SMT Encoding of First Order Logic Formula}
\label{sec:smtencoding}
As summarized in \cref{algo:proofSummary}, our proof discharge algorithm solves a proof obligation $P: \lhs{} \Rightarrow \rhs{}$,
through a sequence of queries $P_i : \lhs{}_i \Rightarrow \rhs{}_i$ to off-the-shelf SMT solvers.
Recall that $P$ may contain \recursiveRelations{}.
However, our algorithm ensures that each $P_i$ is free of \recursiveRelations{} and only contain
scalar equalities.
We encode each query $P_i$ in SMT logic with bitvector and array theories.
In this section, we describe the process of encoding a proof obligation $P_i$ into SMT logic.
We begin by converting $P_i : \lhs{}_i \Rightarrow \rhs{}_i$ into its canonical form $\hat{P}_i$
(as described in \cref{sec:canonicalalgo}).
Although $\hat{P}_i$ does not contain \recursiveRelations{}, it may still contain
ADT variables alongside {\em accessor} and {\em sum-is} expressions.
Due to canonicalization, all top-level {\em accessor} and {\em sum-is} expressions must be of the form
$\prodAccess{v}{a_1,a_2,.,a_n}$ and $\sumIs{\prodAccess{v}{a_1,a_2,.,a_n}}{\textnormal{V}}$ respectively.
We call such an expression $e$ {\em flattenable} and the ADT variable $v$ is called the {\em index} of $e$.
$\hat{P}_i$ is lowered into an intermediate expression $P_i^f$ through a process called {\em flattening}.
This involves `flattening' all flattenable expressions to variables such that
$P_i^f$ only contains scalar values with scalar and memory operations (but importantly not ADT values).
The flattening process is described below.

\begin{enumerate}
\item For each top-level {\em accessor} expression $e = \prodAccess{v}{a_1,a_2,.,a_n}$, we replace it with a
variable named $v \strcat{} \field{a_1} \strcat{} \field{a_2} \strcat{} \dots \strcat{} \field{a_n}$,
where \strcat{} concatenates two strings with a `\strsep{}' character in between i.e.
$"a" \strcat{} "b" = "a \strsep{} b"$.

\item For each ADT $T$ with data constructors $V_1,V_2,\dots,V_k$,
we define an enumeration type $\mathcal{E}(T)$ in SMT logic with items
$\mathcal{E}(V_1),\mathcal{E}(V_2),\dots,\mathcal{E}(V_k)$ respectively.
In the canonical form, each {\em sum-is} expression $e$ must operate on a pseudo-variable.
The last step guarantees that $e$ must be of the form: $e = \sumIs{v}{\textnormal{V}}$.
We replace $e$ with the its SMT equivalent: $(v \strcat{} tag) = \mathcal{E}(V)$
\footnote{\SpecL{} does not allow naming a field of a data constructor \field{tag}.
This prevents collision between variable names obtained due to flattening.}.
\end{enumerate}

For example, the canonical expression $a + \prodAccess{l}{val}$ flattens to $a + l \strsep{} val$.
Similarly, $(\sumIs{\prodAccess{l}{tail}}{LCons})$ flattens to $l \strsep{} tail \strsep{} tag = \mathcal{E}(\cons{LCons})$.
Due to flattening, each flattenable expression $e$ in $\hat{P}_i$ with index $v$ gets lowered
into a variable in $P_i^f$ whose name begins with $v \strsep$.
For the ADT variable $v$, let $\mathcal{F}(v)$ be the set of all such variables in $P_i^f$.
For example, flattening of an expression with $\prodAccess{l}{val}$ and $\sumIs{l}{LCons}$
results in $\mathcal{F}(l) = \{ l \strsep{} val, l \strsep tag \}$.
Importantly, $P_i^j$ may only contain scalar and memory operations (but not ADT values).

Scalar types and their operations map one-to-one to their SMT equivalents.
The memory element \mem{} is represented as a byte-addressable (i.e. \type{i8}) array.
A memory load \memRead{\mem{}}{a}{T} is expanded into the concatenation of \sizeof{T} {\em array-select} operations.
A memory write \memWrite{\mem{}}{a}{v}{T} is expanded into \sizeof{T} nested {\em array-store} operations.

\subsubsection{Reconciliation of Counterexamples}
\label{sec:cerecons}
As detailed in \cref{sec:smtencoding}, each ADT variable $v$ gets lowered into a set of scalar
variables $\mathcal{F}(v)$ during SMT encoding.
Evidently, the models returned by SMT solvers map these variables (in $\mathcal{F}(v)$ instead of $v$)
to constant values.
We are interested in recovering a counterexample for the original query from a
model returned by the SMT solver.
Recall that, these counterexamples help guide the correlation search (in \cref{sec:searchalgo})
and invariant inference (in \cref{sec:invinferalgo}) procedures.
The process of constructing a constant for $v$ from the constant values returned for $\mathcal{F}(v)$
by an SMT solver is called {\em reconciliation}.
Obviously, the reconciled counterexample must be a valid counterexample to the original proof obligation.
$Reconcile(v:T, \gamma)$ is responsible for performing reconciliation for variable $v$ (of type $T$)
from the model $\gamma$ (returned by a SMT solver).
$Rand(T)$ returns an arbitrary constant of type $T$.
For example, consider the rather contrived proof obligation $P: {\tt true} \Rightarrow \sumIs{l}{\cons{LNil}}$.
Clearly, any valuation of $l$ where $l$ is a non-empty list is a valid counterexample to $P$.
However, a counterexample $\gamma$ returned by an SMT solver must contain
the mapping $\{ l \strsep{} tag \mapsto \mathcal{E}(\cons{LCons}) \}$.
During reconciliation, we find that $l \strsep{} tag$ is mapped to the data constructor \cons{LCons}
and recurse for each of its fields \cons{val} and \cons{tail}.
Since $\gamma$ do not contain a mapping for either of these fields, we soundly generate random constants
for these instead.
Note that $Reconcile$ correctly constructs a non-empty but otherwise arbitrary list for $l$, which
is indeed a counterexample to $P$.

\input{chapters/figures/figReconcile.tex}

\subsubsection{Value Graph Representation}
\label{sec:valuegraph}
This section introduces a graphical representation of expressions which encodes both its canonical value and
its deconstruction program at the same time.
We call this the `Value Graph' and we use $\mathcal{V}(e)$ to represent the value graph of expression $e$.
The value properties allow us to consolidate the following procedures used during conversion of a proof obligation
{\em with} \recursiveRelations{} into its canonical form:
(a) decomposition of \recursiveRelations{}, (b) over- and under-approximation of \recursiveRelations{},
and (c) canonicalization of a proof obligation without \recursiveRelations{} (required for encoding in SMT logic).
On the program side, instead of checking equivalence of the deconstruction programs for a \recursiveRelation{} $l_1 \indEq{} l_2$,
we check `equivalence' of the value graphs $\mathcal{V}(l_1)$ and $\mathcal{V}(l_2)$ instead.

We begin with a formal description of ADTs.
This allows us to introduce a graphical representation of ADTs, which is similar to the Value Graph representation, but simpler.
Recall that ADTs are simply `sum of product' types where each data construction represents a variant (of the sum-type) and
each data construction contains values for each of its fields (of the product-type).
On top of ADTs, \SpecL{} also has built-in scalar types: \type{unit}, \type{bool} and \type{i<N>}.
Types in \SpecL{} can be represented in {\em first order recursive types} using the product ($\times$) and sum ($+$) type
constructors; and the scalar types (i.e. nullary type constructors).
The grammar for this type system is given by \typegrammar{} as follows:

$T \rightarrow \mu \alpha. \ T \ |\ T \times \dots \times T \ |\  T + \dots + T \ |\  \type{unit} \ |\ \type{bool} \ |\  \type{i\langle N \rangle} \ |\ \alpha$

For example, the \type{List} type can be written as $\mu \alpha. \type{unit} + (\type{i32} \times \alpha)$.
Note the use of a type variable $\alpha$ which is bound using $\mu$ to represent recursion.
Consider the mutually-recursive types $\type{Tree} = \cons{TNil} | \cons{TCons}(\type{i32}, \type{Forest})$
and $\type{Forest} = \cons{FNil} | \cons{FCons}(\type{Tree}, \type{Forest})$.
\type{Tree} represents a generic \type{i32} tree datatype: either its empty or it contains
a value along with a {\em list of trees} for its children (represented by \type{Forest}).
\type{Tree} can be represented as a {\em closed} term in \typegrammar{} as:
$\mu \alpha. \type{unit} + (\type{i32} \times (\mu \beta. \type{unit + (\alpha \times \beta)}))$,
where $\alpha$ and $\beta$ stand for the recursive types \type{Tree} and \type{Forest} respectively.
A term $t$ is closed iff each type variable $\alpha$ in $t$ is bound (using $\mu$).

\input{chapters/figures/figTypeTrees.tex}

\Cref{fig:typetrees} shows the graphical representation of closed types in \typegrammar{}.
Each internal node represents either a product (\circled{$\times$}) or a sum (\circled{$+$}) type constructor.
The leaf nodes are the scalar types.
For each bound type variable, a backedge is created to the ancestor at which it is bounded.

\input{chapters/figures/figTypeTreesPeel.tex}

\input{chapters/figures/figValueTrees.tex}

Recall that types are equirecursive in \SpecL{}.
In equirecursive typing, two types $t_1$ and $t_2$ (in \typegrammar{}) are equal
iff the infinite trees created by unfolding their graphical representation are equal.
First define unfolding, also define peeling
2 lines about equality using canonical forms which can be easily found.
then the new figure showing the 3 versions of list datatype.

then define value graph and show a figure of 3 cases i alrdy listed.

then go straight into the conversion algorithm
quite easy for most
some time on if-then-else
some times on the variable + lifting constructor
straight into recursive relations
decomposition
approximations
then deconstruction program alternative
unification using the example
give the same example with points-to analysis on it
write the final algorithm
DONE
(will need like 5-6 days to write all this out with figures)
but for now, i can atleast try to do as much as possible so sir can start to review it